---
title: "Data Visualization Notes"
author: "Andrew Nalundasan"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: true
---

```{r setup, include=FALSE}
## Set the default size of figures
## By default, show code for all chunks in the knitted document,
## as well as the output. To override for a particular chunk
## use echo = FALSE in its options.
knitr::opts_chunk$set(fig.width = 8, fig.height = 5, echo = TRUE)

## Load the libraries we will be using
library(gapminder)
library(tidyverse)
library(socviz)
library(here)  # make it easier to work with files and subfolders while not needing to type full paths
library(vtable)
library(ggrepel)  # easier to work with text labels rather than using geom_text()
library(broom)  # to take model objects and turn pieces of them into data frames to use in ggplot
library(survival)  # survival data
library(margins)  # to make partial or marginal effects plots
library(survey)  # to use functions from this package
library(srvyr)  # to use functions from survey and compatible with dplyr
library(coefplot)  # to explore default plots rather than using ggplot
library(GGally)  # to produce generalized pair plots (visual correlation matrix)
library(maps)  # to plot maps
library(cowplot)  # to apply 'theme_map()' to maps
library(statebins)  # to plot statebin maps
library(viridis)  # to use scale functions and color palettes
library(RColorBrewer)  # to use color palettes
library(dichromat)  # to use color palettes that are color-blind friendly
library(ggthemes)  # to use different themes
library(ggridges)  # to use theme_ridges()
library(hrbrthemes)  # to use different themes
```

```{r install, eval = FALSE}

## This code will not be evaluated automatically.
## (Notice the eval = FALSE declaration in the options section of the
## code chunk)

# my_packages <- c("tidyverse", "broom", "coefplot", "cowplot",
                 "gapminder", "GGally", "ggrepel", "ggridges", "gridExtra",
                 "here", "interplot", "margins", "maps", "mapproj",
                 "mapdata", "MASS", "quantreg", "rlang", "scales",
                 "survey", "srvyr", "viridis", "viridisLite", "devtools")

# install.packages(my_packages, repos = "http://cran.rstudio.com")

```

# Set Up Project and Load Libraries

To begin we must load some libraries we will be using. If we do not load them, R will not be able to find the functions contained in these libraries. The tidyverse includes ggplot and other tools. We also load the socviz and gapminder libraries.

# Look at Data

+ The graphs you make are meant to be looked at by someone
+ When making graphs, there is only so much that R can do to keep me on the right track
+ Need to begin cultivating my own good sense about graphs now
+ Perceptual tendencies can be honestly harnessed to make our graphics more effective
+ **Scatterplot** - shows the relationship between two quantities
+ What makes figures bad? 3 varieties:

    + Aesthetic
    + Substantive
    + Perceptual

+ Avoid content-free decoration, including chartjunk aka CLUTTER
+ Visually unique, "infographic" - style graphs are more memorable than more standard statistical visualizations
+ median > raw data

    + Rather look at the trend of an average score, rathern than the trend for the highest possible score
    
+ Relative comparisons need a stable baseline
+ Our ability to distinguish shades of brightness is not uniform
+ **Luminance** - brightness
+ **Chrominance** - intensity
+ **Diverging Scale** - where the steps away from the midpoint are perceptually even in both directions
+ **Qualitative Palette** - easiliy distinguishable colors but also have the same valence for the viewer
+ We should not pick colors in an ad hoc way
+ **Shape** and **color** are two distinct *channels* that can be used to encode information visually
+ Gestalt principles: 

    + *Proximity* - things that are spatially near to one another seem to be related
    + *Similarity* - Things that look alike seem to be related
    + *Connection* - Things that are visually tied to one another seem to be related
    + *Continuity* - Partially hidden objects are completed into familiar shapes
    + *Closure* - Incomplete shapes are perceived as complete
    + *Figure and ground* -  Visual elements are taken to be either in the foreground or in the background
    + *Common fate* - Elements sharing a direction of movement are perceived as a unit
    
+ A scatterplot is a visual *representation* of data, not a way to magically transmit pure understanding
+ There are better and worse ways of visually representing data when the task the user must perform involves estimating and comparing values within the graph
+ We tend to misjudge quantities encoded as *angles*
+ Often, the main audience for your visualizations is myself
+ Working with ggplot():

    1. Must give some information to the ggplot() function
    2. Must choose a geom_() function

# Get Started

+ *Install* package only once, but load the library() each session
+ In R, everything we deal with has a name
+ Almost everything is some kind of object
+ c() is a function: for 'combine' or 'concatenate'

    + Take a sequence of comma separate things inside the () and join them together into a vector where each element is still individually accessible
    
+ Function: special kind of object that can perform actions for me

    + Produces output based on the input it receives
    
```{r}
c(1, 2, 3, 1, 3, 5, 25)  # sends to console and output is provided

# assign to objects
my_numbers <- c(1, 2, 3, 1, 3, 5, 25)
your_numbers <- c(5, 31, 71, 1, 3, 21, 6)
```

## You do things using functions

```{r}
# practice with functions
mean(x=my_numbers)
mean(x=your_numbers)
mean(my_numbers)

my_summary <- summary(my_numbers)
my_summary
```

+ Packages save you from reinventing the wheel
+ Things are done in R by creating and manipulating named objects
+ outputs are as expected

```{r}
# operations
table(my_numbers)
sd(my_numbers)
my_numbers * 5
my_numbers + 1
my_numbers + my_numbers  # vectorized operation - vectors of the same length
```

+ *Vectorized* operation - vectors of the same length are added together (manipulated somehow)

```{r}
# practice with classes
class(my_numbers)
class(my_summary)
class(summary)

# manipulate classes
my_new_vector <- c(my_numbers, "Apple")
my_new_vector
class(my_new_vector)
```

+ Adding "Apple" changes the entire vector from *number* type to *character* type
+ Character strings cannot be used in calculations


+ *Matrix* - consists of rows and columns of numbers
+ *Data Frame* - rectangular table consisting of rows (of observations) and columns (of variables)

    + Columns can be of different classes

```{r}
titanic
class(titanic)

# selecting particular elements in a df
titanic$percent
```

+ *Tibble* - 

    + used to store variables of different classes all together in a single table of data
    + Do more to let us know about what they contain
    + More friendly when interacted with from the console
    
```{r}
titanic_tb <- as_tibble(titanic)
titanic_tb
```

+ To see inside an object, ask for its *structure*
+ *vector* is just a sequence of numbers

```{r}
str(my_numbers)
str(my_summary)
```

## Get data into R

```{r}
url <- "https://cdn.rawgit.com/kjhealy/viz-organdata/master/organdonation.csv"

organs <- read_csv(file=url)
```

+ "Column specification" - tells us that read_csv() function has assigned a class to each column of the object it created from the CSV file
+ It is helpful to know what class each column / variable is

    + A variable's class determines what sort of operations can be performed on it
    
+ read_csv() will not classify variables as *factors* unless it's told to
+ You will need to take care that any labeled variables imported into R are coded properly, so that you do not end up mistakenly using missing data in your analysis
+ "Tidy" format:

    + *long* rather than *wide* format
    + Every observation a row
    + Every variable a column
    
## Make first figure
    
```{r}
gapminder
p <- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp))
p + geom_point()
```

# Make a Plot

+ Visualizing data with ggplot *always* involves the same sequence of steps
+ The way you use ggplot to think about the logical structure of the plot
+ *aesthetics* - logical connections between the data and the plot elements

## How Ggplot works 

+ Steps to creating ggplots: 

    1. Tell ggplot() function what the data is and how the variables in the data logically map onto the plot's aesthetics
    2. Take the result and say what general sort of plot wished to create, or *geom*
    
+ Plots: 

    + geom_point() - scatterplot
    + geom_bar() - bar plot
    + geom_boxplot() - boxplot
    
+ ggplot() + geom() yields a plot
+ the rest of it is just details

+ Specify the details: 

    + scales
    + labels
    + legends
    + axes
    + etc.

## Tidy Data

+ Tidy data:

    + *long-format* - every variable is a column, every observation is a row
    + *wide-format* - some variables are spread out across columns
    + ggplot wants data in *long-format*
    
+ tidy data is much more straightforward to work with when it comes to specifying the mappings that you need to coherently describe plots

## Mappings link data to things you see

```{r}
# look at the data
head(gapminder, 10)
```

+ Plot life expectancy against per capita GDP for all country-years in the data

```{r}
p <- ggplot(data = gapminder)

```

+ data still needs to be mapped
+ *mapping* - tell ggplot which variables in the data should be represented by which visual elements in the plot
+ ggplot also doesn't know what sort of plot we want (*geom*)

```{r}
p <- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp))
```

+ here, we've given ggplot 2 arguments: 

    1. data
    2. mapping
    
+ the *data* argument tells ggplot where to find the variables it is about to use

    + any mentions of variables will be searched for in the specified *data*
    
+ *mapping* argument is not a data object, not even a character string. It's a *function*
+ this aes() function call says:

    + "The variable on the x-axis is going to be *gdpPercap* and the variable on the y-axis is going to be *lifeExp*"
    
+ *mapping = aes(...)* argument *links variables to things you will see* on the plot
+ *mapping* does not directly say what particular colors or shapes will be on the plot
+ *mapping* says which *variables* in the data will be *represented* by visual elements on the plot
+ we need to add a *layer* to the plot

```{r}
p + geom_point()
```

## Build plots layer by layer

+ Conceptually, we will always follow the same set of steps:

    1. Tell the ggplot() function what our data is
    2. Tell ggplot() *what* relationships we want to see. For convenience we will put the results of the first two steps in an object called p
    3. Tell ggplot *how* we want to see the relationships in our data
    4. Layer on geoms as needed, by adding them to the p object one at a time
    5. Use some additional function sto adjust scales, labels, tickmarks, titles. 
    
+ default plots - coordinate system typically cartesian
+ *cartesian* coordinate system - plane defined by an x-axis and a y-axis
+ typically in R - *functions* take *objects* as *inputs* and produce *objects* as *outputs*

```{r}
p <- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp))
p + geom_smooth()
```

+ geom_smooth() calculated a smoothed line for us and shaded in a ribbon showing the standard error for the line

```{r}
p <- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp))
p + geom_point() + geom_smooth()
```

+ see the data points and the smoothed line together. 
+ R console returns a message regarding "gam". 

    + R has fit a generalized additive model
    + This implies that there are other models that geom_smooth() understanding
    + let's try adding method = 'lm' (linear model)


```{r}
p <- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp))
p + geom_point() + geom_smooth(method = 'lm')
```

+ It's possible to give geoms separate instructions that they will follow
+ Let's try to take the log to spread out the distribution on the left side of this plot
+ 'scale_x_log10()' - scales the x-axis of a plot to a log 10 basis


```{r}
p <- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp))
p + geom_point() + geom_smooth(method = 'gam') + scale_x_log10()
```

+ This plot is different from in the book. 
+ To match the plot in the book, change to 'method = 'lm''.
+ scale_ functions can be applied to format the tick-marks

+ Gain access to all functions within a library, load the library via library()
+ Use just 1 function from a library, use :: to grab that specific function
+ syntax - 

    + thepackage::thefunction
    
```{r}
p <- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp))
p + geom_point() + 
  geom_smooth(method = 'lm') + 
  scale_x_log10(labels = scales::dollar)
```

+ scale transformations: 

    1. Possible to directly transform x- or y- axis by adding something like "scale_x_log10()" or "scale_y_log10()" to the plot
        + The x- or y- axis will be transformed
        + By default, the tick-marks on the axis will be labeled using *scientific notation*
    2. Possible to gives these "scales_" functions a "labels" argument that reformats the text printed underneath the tick-marks on the axes.
    
## Mapping aesthetics vs setting them

+ *aesthetic mapping* - specifies that a variable will be expressed by one of the available visual elements, such as size, color, or shape

```{r}
p <- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp, color = continent))
```

+ this command says "the portery 'color' will represent the variable *continent*"
+ aka - "color will map *continent*
+ if we wanted all the points to be purple, it is NOT done via *mapping* function

```{r}
p <- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp, color = "purple"))
p + geom_point() + geom_smooth(method = "loess") + scale_x_log10()
```

+ *aesthetic* - mapping of variables in your data to properties you can see on the graph
+ aes() function is where the mapping is specified, and the function is trying to do its job

    + aes() is trying to treat "purple" as if it were a variable
    + a variable should have as many observations as there are rows in the data
    + aes() is for mappings only - do not use to change properties in a particular value
    
```{r}
p <- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp))
p + geom_point(color = "purple") + geom_smooth(method = "loess") + scale_x_log10()
```

+ R knows what color "purple" is when passed into geom_point()
+ R doesn't know what color "purple" is when passed into the *aesthetic*

```{r}
p <- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp))
p + geom_point(alpha = 0.3) + geom_smooth(color = "orange", se = FALSE, size = 2, method = "lm") + scale_x_log10()
```

+ updated size from 8 to 2
+ *alpha* controls how transparent the object will appear when drawn. From scale 0-1
+ transparency helps to make it easier to see where the bulk of observations are located

```{r}
p <- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp))
p + geom_point(alpha = 0.3) + 
  geom_smooth(method = "gam") + 
  scale_x_log10(labels = scales::dollar) + 
  labs(x = "GDP Per Capita", y = "Life Expectancy in Years",
       title = "Economic Growth and Life Expectancy", 
       subtitle = "Data points are country-years", 
       caption = "Source: Gapminder.")
```

+ Make a nicer plot: 
    
    1. Set *alpha* of the points to a low value
    2. Make nice x- and y- axis labels
    3. Add a title, subtitle, and caption
    
+ Unless told otherwise, all geoms layered on top of the original plot object will inherit that object's mappings (everything inherits from "p")

```{r}
p <- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp, color = continent))
p + geom_point() + geom_smooth(method = "loess") + scale_x_log10()
```

+ standard error ribbons are all gray. can be adjusted using *fill* 
+ *color* aesthetic affects appearance of lines and points
+ *fill* is for the filled areas of bars, polygons, and standard error ribbon

```{r}
p <- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp, color = continent, fill = continent))
p + geom_point() + geom_smooth(method = "loess") + scale_x_log10()
```

+ Legend: the smoother understand both *color* (for the line itself) and *fill* (for the shaded standard error ribbon)

## Aesthetics can be mapped per Geom

+ By default, geoms inherit their mappings from ggplot()

```{r}
p <- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp))
p + geom_point(mapping = aes(color = continent)) + 
  geom_smooth(method = "loess") + 
  scale_x_log10()
```

+ Legend : the points have *color*

    + Colored line and shaded box are both absent
    + We only see a legend for the mapping of *color* to *continent* in *geom_point()
    
+ Smoothing line drawn by geom_smooth() is set by default to bright blue with default shaded ribbon to gray

```{r}
p <- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp))
p + geom_point(mapping = aes(color = log(pop))) + scale_x_log10()
```

+ Also possible to map continuous variables (rather than just categorical variables) to the *color* aesthetic too
+ When taking the log() of some data, ggplot produces a gradient scale

## Save your work

+ control size and format of a code chunk: 

    + #knitr::opts_chunk$set(fig.width = 8, fig.height = 5)
    + this sets default size of plots within the .Rmd document
    + this indicates 8x5 figures
    
```{r example, fig.width = 12, fig.height = 9}
p + geom_point()

# save the most recent plot
ggsave(filename = "../03_visuals/my_figure.png")
ggsave(filename = "../03_visuals/my_figure.pdf")

```

+ Easiest way to save figures when using ggplot() is to call ggsave()
+ Andrew's tip: set working directory to "03_visuals" folder before calling ggsave()

    + When working director is set to this folder, ggsave() will save the file in this folder directly
    + When running code directly from .Rmd, use folder path trick to save files to the intended folder
        + "../03_visuals/filename.png"
        
```{r}
p_out <- p + geom_point() + geom_smooth(method = "loess") + scale_x_log10()

ggsave("../03_visuals/my_figure.pdf", plot = p_out)

# save file using here()
ggsave(here("03_visuals", "lifeexp_vs_gdp_gradient.pdf"), plot = p_out)
```

+ useful to have a separate folder for just visuals
+ name figures similar to variables and objects - make them meaningful
+ do not include special characters in figure file names including: 
    
    + ', `, ' ', /, \, "
    
+ save file using "here()" and specify which folder within the folder structure: "03_visuals"

+ Important distinction between *vector* format vs. *raster* format
    
    + *vector* format:
        + PDF or SVG file types
        + stored as a set of instructions about lines, shapes, colors, and their relationships
        + Viewing software: Adobe Acrobat / Apple Preview apps for PDFs
        + Can be resized without being distorted
        + Good choice for high quality images
    + *raster* format: 
        + JPG and PNG file types
        + stored images as a grid of pixels of predefined size with information about the location, color, brightness, of each pixel in the grid
        + raster cannot be easily resized without being pixelated or distorted
        + JPG and PNG standard file types for images on the web
        
+ Important to know what the figure will be used for and in what setting/medium of communication

```{r}
# use ggsave() to explicitly set the height and width of the plot in the chosen units
ggsave(here("03_visuals", "lifeexp_vs_gdp_gradient.pdf"), plot = p_out, height = 8, width = 10, units = "in")
```

## Where to go next

1. Explore with ordering of geom_point() and geom_smooth(). How is this helpful with drawing plots?

```{r}
p + geom_point()
p + geom_smooth()
p + geom_point() + geom_smooth()
# geom_smooth() is layered ABOVE geom_point()
p + geom_smooth() + geom_point()
# geom_smooth() is layered BELOW geom_point()
```

**Comments**
+ the last geom element is layered on top. It's like layering layers on top of one another. First layer in is on the bottom. Last geom layer called is at the top of the stack. 
+ this would be useful if I know which layer I need to stand out the most, and which layers will be used as background to help build the story

2. Change mappings of aes() function. What does switching up the mappings tells me about the units of observation in the dataset?

```{r}
p <- ggplot(data = gapminder, mapping = aes(x = pop, y = lifeExp))
p + geom_point() + 
  geom_smooth() +
  scale_x_log10()
```

**Comments**

+ when plotting lifeExp against pop rather than GDP, it appears that the distribution of observations resemble a random distribution such as poisson. 
+ geom_smooth is almost flat. It is a slightly positive trend
+ I don't know what this tells me about the unit of observation in the dataset 

3. Try alternative scale mappings like scale_x_sqrt() and scale_x_reverse()

```{r}
p <- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp))

# scale_x_log10()
p + geom_point() + 
  geom_smooth() + 
  scale_x_log10() + 
  ggtitle("log x 10")

# scale_x_sqrt()
p + geom_point() + 
  geom_smooth() + 
  scale_x_sqrt() + 
  ggtitle("log x sqrt")


# scale_x_reverse()
p + geom_point() + 
  geom_smooth() + 
  scale_x_reverse() +
  ggtitle("log x reverse")

# scale_y_log10()
p + geom_point() + 
  geom_smooth() + 
  scale_y_log10() +
  ggtitle("log y 10")

# scale_y_sqrt()
p + geom_point() + 
  geom_smooth() + 
  scale_y_sqrt() +
  ggtitle("log y sqrt")

# scale_y_reverse()
p + geom_point() + 
  geom_smooth() + 
  scale_y_reverse() +
  ggtitle("log y reverse")

```

**Comments**

+ scale_x_log10():

    + takes the log of the x-axis and spreads out the observations that are clumped up

+ scale_x_sqrt():

    + I guess instead of taking the log of the x axis, this takes the sqare root of the x axis
    + x-axis values are a lot smaller than the log
    + points are clumped up slightly towards left of the plot

+ scale_x_reverse():

    + makes x-axis read in reverse (origin is at bottom right corner rather than bottom left corner)
    + plotted with default settings to x-axis, just flipped (no log and no square root applied)
    + not sure when reversing would make sense

+ scale_y_log10():

    + this doesn't seem to take the log of the y-axix. 
    + this looks similar to the default plot on the y-axis - observations hella clumped up

+ scale_y_sqrt():

    + not much difference from y_log scaling - observations just a little spread out
    + more of a curvy shape compared to log
    + not as drastic a change as between x_log10 vs. x_sqrt


+ scale_y_reverse():

    + as expected, y-axis is reversed
    + origin is at top left rather than bottom left
    + not sure when reversing would make sense
    
4. Map color to year instead of continent. What are the results? Think about what class the object year is. 

```{r}
p <- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp))

# color mapped to continent
p + geom_point(mapping = aes(color = continent)) + 
  geom_smooth(method = "loess") + 
  scale_x_log10()

# color mapped to year
p + geom_point(mapping = aes(color = year)) + 
  geom_smooth(method = "loess") + 
  scale_x_log10()

```

**Comments**
+ color by year turned the color scheme to a gradient
+ dark colors for early in time, light colors more recent in time
+ class of year is "integer"
+ class of continent is "factor"
+ we get gradient of color because integer is a continuous variable
+ when color is mapped to a factor variable, we get discrete classes of each factor

5. What happens when mapping color to "factor(year)"?

```{r}
p + geom_point(mapping = aes(color = factor(year))) + 
  geom_smooth(method = "loess") + 
  scale_x_log10()
```

**Comments**
+ when mapping color to factor(year), R changes the "year" variable from an integer to a factor variable. 
+ R then plots every year as a separate factor and we end up with a factor for every single year
+ might as well still be a gradient because there are so many

6. Improve figure 3.13. What are we gaining and losing by ignoring the temporal and country-level structure of the data? 

```{r}
p <- ggplot(gapminder, mapping = aes(x = gdpPercap, y = lifeExp))
p + geom_point(alpha=0.3) + 
  geom_smooth(method="lm") + 
  scale_x_log10(labels = scales::dollar) + 
  labs(x = "GDP Per Capita", y = "Life Expectancy in Years", 
       title = "Economic Growth and Life Expectancy", 
       subtitle = "Data points are country-years", 
       caption = "Source: Gapminder.")

```

**Comments**
+ what does "country-years" mean?
+ perhaps we could add a layer to show countries or continents?
+ there are 142 countries in the gapminder dataset. That's too many to display

**Chapter Wrap Up**
+ It's always worth trying something
+ main flow of ggplot is always the same

    1. start with a table of data
    2. map the variables I want to display to aesthetics like position, color, or shape
    3. Choose 1 or more geoms to draw the graph


# Show the Right Numbers

## Introduction

+ Focus on ggplot's central workflow
+ *geoms* - functions that make particular kinds of plots
+ common problem - when an aesthetic is mistakenly set to a *constant value* instead of being mapped to a *variable*
+ *grouping* - internal structure of the data
+ *faceting* - break up data into pieces for a plot
+ *transforming* - get ggplot to perform some calculations on or summarize the data before producing the plot

## Colorless Green Data Sleeps Furiously

+ grammar of graphics - set of rules for producing graphics from data, taking pieces of data and mapping them to geometric objects that have aesthetic attributes together with further rules for transforming the data if needed, adjusting scales, and projecting the results onto a different coordinate system
+ syntax and semantics will make or break your plots

## Grouped Data and the "Group" Aesthetic

+ Plot the trajectory of life expectancy over time for each country in gapminder
+ geom_line() - will draw lines by connecting observations in order of the variable on the x-axis

```{r}
p <- ggplot(gapminder, mapping = aes(x=year, y=gdpPercap))
p + geom_line()
```

**Comments**
+ This is not what was expected. 
+ This looks like a bar chart
+ ggplot does not know that the yearly observations in the data are grouped by country - I must specify this
+ in this plot, geom_line() tries to join up all the lines for each particular year in the order they appear in the dataset
+ ggplot does not know that the very first observation belongs to Afghanistan in 1952
+ ggplot does not know to look for the second observation for Afghanistan in 1953
+ ggplot plots out all 1952 observations moving alphabetically (Afghanistan -> Zimbabwe) before moving to the next year
+ when bugs occur in ggplot, the reason is almost always that something has gone wrong in the mapping between the data and aesthetics for the gom being used

Use the "group" aesthetic to tell ggplot explicitly about this country-level structure

```{r}
p <- ggplot(gapminder, mapping = aes(x=year, y=gdpPercap))
p + geom_line(aes(group = country))
```

**Comments**
+ still hella rough, but data is displaying correctly
+ each line represents the trajectory of a country over time
+ giant outlier is Kuwait
+ "group" aesthetic usually only needed when the *grouping* information you need to tell ggplot about is not built into the variables being mapped
+ grouping is already clear for categorical/factor variables
+ when mapping x to "year", there is no information in "year" that tells ggplot that it is grouped by country

## Facet to Make Small Multiples

+ *facet* - technique that allows a lot of information to be presented compactly and in a consistently comparable way
+ *facet* - way of organizing a series of geoms
+ R's "formula" syntax when using the "tilde character": ~
+ term in the formula: the variable we want the data to be broken up by

```{r}
p <- ggplot(gapminder, mapping = aes(x=year, y=gdpPercap))
p + geom_line(aes(group = country)) +
  facet_wrap(~continent)
```

**Comments**
+ using facet wrap minimizes the duplication of axis labels and other scales
+ use "ncol" to control the number of columns used to lay out the facets
+ can layer on additional geoms if needed

```{r}
p <- ggplot(gapminder, mapping = aes(x=year, y=gdpPercap))
p + geom_line(color="gray70", aes(group = country)) +
  geom_smooth(size = 1.1, method = "loess", se = FALSE) +
  scale_y_log10(labels=scales::dollar) +
  facet_wrap(~ continent, ncol = 5) +
  labs(x = "Year", 
       y = "GDP per capita", 
       title = "GDP per capita on Five Continents")
```

**Comments**
+ summary of plot:

    + brings together a aesthtic mapping of x and y variables
    + brings together a grouping aesthetic (country)
    + brings together 2 geoms (lineplot + smoother)
    + y-axis is log-transformed with appropriate tick labels
    + brings together a faceting variable (continent)
    + includes axis labels and a title
    
+ facet_wrap() is best used when wanting to facet something based on a **single categorical variable**
+ if cross-classifying data by 2 categorical variables, use **facet_grid()**

## Working with GSS Data

+ gapminder:
    
    + gapminder consists mostly of *continuous* variables
    + *continuous* variables can take any value across a large range and vary smoothly
    + only *categorical* variable is "continent"
        + unordered *categorical* variable
        + each country belongs to a continent, but the continents themselves have no natural ordering (other than alphabetical)
        
+ GSS:

    + long-running survey of American adults that asks about a range of topics of interest to social scientists
    + contains many *categorical* measures
    + unordered *categorical* data:
        + ethnicity
        + sex
    + ordered *categorical* data: 
        + educational attainment
        + opinion questions asked in yes/no terms
        + questions answered on a point scale, with "neutral" in the middle
    + *numeric* variables
        + number of children - range of integers in a narrow range
            + could also be represented on a scale with option like "6+ children"
    + *continuous* variables are often obtainable only as ordered categories
        + income
            + 0-30k, 31-60k, 61-90k, etc.
            
```{r}
# look at the data
glimpse(gss_sm)
head(gss_sm, 10)
summary(gss_sm)
vtable(gss_sm)  # vtable is just a pretty version of glimpse, populates in "Viewer" pane
class(gss_sm)
```

Make a smoothed scatterplot of the relationship between the age of the respondent and the number of children they have

```{r}
p <- ggplot(gss_sm, mapping = aes(x = age, y = childs))
p + geom_point(alpha = 0.2) +
  geom_smooth() +
  facet_grid(sex ~ race)
```

**Comments**
+ multipanel layouts are especially effective when used to summarize continuous variation acros 2+ categorical variables, with the categories ordered in some sensible way

```{r}
p <- ggplot(gss_sm, mapping = aes(x = age, y = childs))
p + geom_point(alpha = 0.2) +
  geom_smooth() +
  facet_grid(sex ~ race + degree)
```

**Comments**
+ Possible to add to the two-way comparison (sex ~ race + degree)
+ Multiple dimensions of plots like this will quickly become very complicated if thevariables have more than a few categories each

## Geoms Can Transform Data

+ geom_smooth() will add a trendline to a figure

    + methods: 
        + "loess"
        + "lm"
        + Generalized Additive Model

+ some geoms plot our data directly on the figure, like geom_point()
+ every 'geom' function has a default 'stat' function
+ every 'stat' function has a default 'geom' function
+ we sometimes want to calculate different stats using different geoms

```{r}
p <- ggplot(data = gss_sm, mapping = aes(x = bigregion))
p + geom_bar()
```

**Comments**
+ Bar chart provides a count of the number of individual observations in the data set by region of the US
+ y-axis, "count" has bee calculated for me
+ default stat function for geom_bar(): stat_count()
+ stat_count() calculates 2 stats: "count" and "prop" (proportion)

```{r}
p <- ggplot(data = gss_sm, mapping = aes(x = bigregion))
p + geom_bar(mapping = aes(y = ..prop..))
```

**Comments**
+ produces chart of relative frequencies rather than counts by using "prop" statistic instead
+ To make certain these temporary variables won't be confused with others we are working with, their names begin and end with two periods
+ 'dunder period' !!!
+ when calling this, it will look like: 

    + <mapping> = <..statistic..>
    
```{r}
p <- ggplot(gss_sm, mapping = aes(x = bigregion))
p + geom_bar(mapping = aes(y = ..prop.., group = 1))
```

**Comments**
+ this chart shows that the bars *sum* to 1

    + so that we get the number of observations per continent as a proportion of the total number of observations

+ ggplot *ignores* the x-categories when calculating denominator of the proportion and use the total number observations instead
+ specify "group = 1" inside aes() call

    + value of 1 is a "dummy group" that tells ggplot to use the whole dataset when establishing the denominator for its prop calculations
    
```{r}
table(gss_sm$religion)
```

```{r}
p <- ggplot(gss_sm, mapping = aes(x = religion, color = religion))
p + geom_bar()

p <- ggplot(gss_sm, mapping = aes(x = religion, fill = religion))
p + geom_bar() + guides(fill = FALSE)
```

**Comments**
+ "fill" is for painting the insides of shapes
+ mapping "religion" to "color", only the border lines of the bars will be assigned colors, and the insides remain gray
+ we have mapped 2 aesthetics to the same variable

    + both 'x' and "fill" are mapped to "religion"

+ default is to show a legend for the color variable
+ guides() controls whether guiding information about any particular mapping appears or not
+ guides(fill = FALSE) will remove the legend

    + this assumes that the audience does not need the legend to figure out what the figure is telling
    
## Frequency Plots the Slightly Awkward Way

+ use geom_bar() to cross-classify 2 categorical variables
+ Best Practice:  calculate the table first before passing the results along to ggplot to graph
+ geom_bar() - output is controlled by the "position" argument

```{r}
p <- ggplot(gss_sm, mapping = aes(x = bigregion, fill = religion))
p + geom_bar()
```

**Comments**

+ default output is stacked bar chart with counts on the y-axis
+ stacked bar charts - it is difficult for readers on the chart to compare lengths and areas on an unaligned scale
+ better alternative: set "position" argument to "fill" (different from "fill" *aesthetic*)


```{r}
p <- ggplot(gss_sm, mapping = aes(x = bigregion, fill = religion))
p + geom_bar(position = "fill")
```

**Comments**

+ now all bars are same height, which is easier to make comparisons across groups

```{r}
p <- ggplot(gss_sm, mapping = aes(x = bigregion, fill = religion))
p + geom_bar(position = "dodge")
```

**Comments**

+ ggplot places the bars side-by-side as intended but changes the y-axis back to a *count* of observations
+ we wanted proportions, not counts

```{r}
p <- ggplot(gss_sm, mapping = aes(x = bigregion, fill = religion))
p + geom_bar(position = "dodge", mapping = aes(y = ..prop..))
```

**Comments**

+ there seems to be an issue with grouping

```{r}
p <- ggplot(gss_sm, mapping = aes(x = bigregion, fill = religion))
p + geom_bar(position = "dodge", mapping = aes(y = ..prop.., group = religion))
```

**Comments**

+ outputs a bar chart where values of "religion" are broken down across regions, with a proportion showing on the y-axis
+ bars do not sum to 1 within each reagion
+ bars for any particular religion sum to 1 *across* regions
+ nearly 50% of those who said they were Protestant live in the South
+ just over 10% saying they were Protestant live in the Northeast
+ over 50% of Jewish live in the Northeast, compared to 25% in the South


```{r}
p <- ggplot(gss_sm, mapping = aes(x = religion))
p + geom_bar(position = "dodge", mapping = aes(y = ..prop.., 
                                               group = bigregion)) + 
  facet_wrap(~bigregion, ncol = 2)
```

**Comments**

+ don't try to force geom_bar() to do all the work in a single step
+ Ask ggplot to give us a proportional bar chart of religious affiliation, then facet that by region

## Histograms and Density Plots

+ Different geoms transform data in different ways, but ggplot's vocabulary for them is consistent
+ Histogram - way of summarizing a continuous variable by chopping it up into segments or "bins" and counting how many observations are found within each bin

    + must decide how finely to bin the data
    
+ Bar chart - categories are given to us going in
+ dataset: midwest

    + information abour counties in midwestern states
    + area measured in square miles
    
```{r}
p <- ggplot(midwest, mapping = aes(x = area))
p + geom_histogram()
```

**Comments**

+ message: "stat_bin()" using "bins = 30". Pick better value with "binwidth".

    + set number of bins in argument
    + 30 bins chosen by R by default
    
```{r}
p <- ggplot(midwest, mapping = aes(x = area))
p + geom_histogram(bins = 10)
```

**Comments**

+ similar to geom_bar(), new variable "count" auto calculated for y-axis
+ when drawing histograms, it's worthwhile to experiment with bins
+ histograms - summarize single variables

```{r}
# character vector of 2 states
oh_wi <- c("OH", "WI")

# use subset() to take data and filter it so that we select only rows whose "state" name is in this vector
# %in% operator is a convenient way to filter on more than one term in a variable when using subset()
p <- ggplot(data = subset(midwest, subset = state %in% oh_wi),
            mapping = aes(x = percollege, fill = state))
p + geom_histogram(alpha = 0.4, bins = 20)
```

**Comments**

+ subset the data to pick out just 2 states
+ subset() - take our data and filter it so that we select only rows whose "state" name is in this vecotor
+ %in% - filer on more than one term in a variable when using subset()

```{r}
p <- ggplot(midwest, mapping = aes(x = area))
p + geom_density()
```

**Comments**
+ geom_density() - alternative for when working with a continuous variable, an alternative to binning the data and making a histogram 
+ make baselines of the density curves go away: 

    + geom_line(stat = "density")
    + unable to use "fill" when doing this
    
```{r}
p <- ggplot(midwest, mapping = aes(x = area, fill = state, color = state))
p + geom_density(alpha = 0.3)
```

**Comments**
+ many options to use in these arguments
+ count-based defaults computed by the stat_ functions will return proportional measures if asked
+ ..count.. - density * number points

    + this can be used in stacked density plots
    
```{r}
p <- ggplot(data = subset(midwest, subset = state %in% oh_wi),
             mapping = aes(x = area, fill = state, color = state))
p + geom_density(alpha = 0.3, mapping = (aes(y = ..scaled..)))
```

## Avoid Transformation When Necessary

+ geom_bar() does its calculations on the fly using stat_count() behind the scenes to produce the counts or proportions it displays
+ Often, our data is already a summary table

    + This can happen when we have computed a table of marginal frequencies or percentages from the original data
    
```{r}
titanic
```

**Comments**
+ this table already provides percentage values in a summary table
+ we no longer have any need for ggplot to perform calculations

```{r}
p <- ggplot(data = titanic, mapping = aes(x = fate, y = percent, fill = sex))
p + geom_bar(position = "dodge", stat = "identity") + 
  theme(legend.position = "top")
```

**Comments**
+ stat = "identity" tells geom_bar() not to do any default calculations. We're already working with proportions and counts.
+ geom_col() is the same thing as geom_bar(), but assumes "stat = "identity"" as default

    + use this geom when don't need any calculations done on the plot
    
+ stat = "identity" - means "don't do any summary calculations"
+ position = "identity" - "just plot the values as given" 

    + plot a flow of positive and negative values in a bar chart
    + alternative to a line plot
    + often seen in public policy settings where changes relative to some threhold level or baseline are of interest
    
```{r}
oecd_sum
```

```{r}
p <- ggplot(data = oecd_sum, mapping = aes(x = year, y = diff, fill = hi_lo))
p + geom_col() + guides(fill = FALSE) + 
  labs(x = NULL, y = "Difference in Years", 
       title = "The US Life Expectancy Gap", 
       subtitle = "Difference between US and OECD average life expectancies, 1960-2015",
       caption = "Data: OECD. After a char by Christopher Ingraham, 
       Washington Post, December 27th 2017.")
```

**Comments**
+ default action for geom_col() sets stat = "identity" and position = "identity"

## Where to Go Next

1. Experiment with different ways to facet gapminder data.

    + Plot population and per capita GDP while faceting on year
    + Facet on "country" but anticipate heaps of panels
        + Assign plot to an object and save as PDF. Experiement with height and width dimensions
        
```{r}
p <- ggplot(gapminder, mapping = aes(x = year, y = gdpPercap))
p + geom_line(aes(group = country)) + 
  facet_wrap(~ continent) + 
  labs(title = "facet on continent")

p + geom_line(aes(group = country)) + 
  facet_wrap(~ year) + 
  labs(title = "facet on year")

massive_facet <- p + geom_line(aes(group = country)) + 
  facet_wrap(~ year) + 
  labs(title = "facet on country")

ggsave(here("03_visuals", "massive_facet.pdf"), plot = massive_facet)


```

**Comments**

+ easy to get overwhelmed by facets if too many categories to plot and deal with

2. What is difference between "facet_grid(sex ~ race)" vs. "facet_grid(~ sex + race)"

```{r}
p <- ggplot(gss_sm, mapping = aes(x = age, y = childs))
p + geom_point(alpha = 0.2) +
  geom_smooth() + 
  facet_grid(sex ~ race) + 
  labs(title = "facet_grid(sex ~ race)")

p + geom_point(alpha = 0.2) +
  geom_smooth() + 
  facet_grid(~ sex + race) + 
  labs(title = "facet_grid(~ sex + race)")
```

**Comments**

+ "~ sex + race" breaks up the grid to have a separate column for each combination of "sex" and "race"
+ "sex ~ race" breaks up the grid to have one row for each category of "sex" and a separate column for each "race"

3. Investigate difference between "facet_wrap(~ sex + race) vs. "facet_grid(~ sex + race)"

```{r}
p + geom_point(alpha = 0.2) +
  geom_smooth() + 
  facet_wrap(~ sex + race) + 
  labs(title = "facet_wrap(~ sex + race)")

p + geom_point(alpha = 0.2) +
  geom_smooth() + 
  facet_grid(~ sex + race) + 
  labs(title = "facet_grid(~ sex + race)")
```

**Comments**

+ facet_wrap - breaks up "sex" categories into separate rows by default

    + lays out results in a wrapped 1-dimensional table
    
+ facet_grid - by default, outputs 1 row of all data

    + lays out results in a fully cross-classified grid

4. Replace visuals the call "geom_histogram()" with "geom_freqpoly()" instead and see the difference

```{r}
# character vector of 2 states
oh_wi <- c("OH", "WI")

# use subset() to take data and filter it so that we select only rows whose "state" name is in this vector
# %in% operator is a convenient way to filter on more than one term in a variable when using subset()
p <- ggplot(data = subset(midwest, subset = state %in% oh_wi),
            mapping = aes(x = percollege, fill = state))
p + geom_histogram(alpha = 0.4, bins = 20) + 
  labs("geom_histogram()")

p + geom_freqpoly(alpha = 0.4, bins = 20) + 
  labs("geom_freqpoly()")
```

**Comments**

+ geom_freqpoly() draw lines instead of bars (bins) to connect counts. 
+ This is a "frequency polygon"

5. Play with geom_bind2d() to make a histogram with mapped x and y. Use gapminder data. Provide bins for both x and y with "bins = c(20, 50)"

```{r}
p <- ggplot(gapminder, mapping = aes(x = gdpPercap, y = lifeExp))
p + geom_bin2d(bins = c(20, 50)) + 
  geom_smooth(method="lm") + 
  scale_x_log10(labels = scales::dollar) + 
  labs(x = "GDP Per Capita", y = "Life Expectancy in Years", 
       title = "Economic Growth and Life Expectancy", 
       subtitle = "Data points are country-years", 
       caption = "Source: Gapminder.")
```

**Comments**

+ bin2d takes arguments for both x and y
+ plots counts as a gradient since continuous variables 
+ added regression line and log of x-axis to make it nicer

6. Using midwest data, plot % below poverty line (percbelowpoverty) against % college-educated (percollege). Try this with and without a geom_point() layer

```{r}
p <- ggplot(midwest, mapping = aes(x = percollege, y = percbelowpoverty))
p + geom_density_2d() + 
  labs(title = "geom_density_2d()")

p + geom_density_2d() + 
  geom_point() +
  scale_x_log10() +
  labs(title = "geom_density_2d() + geom_point()")

```

**Comments**

+ density_2d is cool! contour lines around the frequency of observations
+ this reminds me of clusters or medoids in ML
    
    + easy to see this with geom_point layer on top of the density_2d

# Graph Tables, Make Labels, Add Notes

+ Added layer of complexity: 

    1. Learn about how to transform data *before* sending it to ggplot (data wrangling)
        + Better to get things into the right shape before working with ggplot
    2. Explore more geoms and learn how to choose between them
        + Given the data we have vs. visualization we want
        + Subset the data before displaying it
    3. Utilize scale, guide, and theme functions
        + Provides more control over content and appearance of visuals
        + Increases legibility to audience
        + Layer geoms on top of one another
    + We will always be building visuals piece by piece / layer by layer
    + * We want a table of tidy data, a mapping of variables to aesthetic elements, and a particular type of graph
    
## Use Pipes to Summarize Data

+ *column marginals* - numbers sum to 100 by column 
+ *row marginals* - numbers sum to 100 across the rows
+ very easy to lose track of whether R has calculated row margins, column margins, or overall relative frequencies
+ Best Practice: make calculations first, THEN make plots
+ *dplyr* - component of the tidyverse that provides functions for manipulating and reshaping tables of data on the fly
+ goal: summary table with % of religious preferences grouped within a region
+ %>% - allows us to start with a data frame and perform a *sequence* or *pipeline* of operations to turn it into another, usually smaller and more aggregated, table

    + data goes in one side of the pipe, actions are performed via functions, and results come out the other

+ 4 typical pipes: 

    1. *Group* - group the data into the nested structure we want for our summary, such as "Religion by Region" or "Authors by Publications by Year"
    2. *Filter* or *Select* - filter or select pieces of the data by row, column, or both. This gets us the piece of the table we want to work on
    3. *Mutate* - mutate the data by creating new variablse at the *current* level of grouping. This adds new columns to the table without aggregating it
    4. *Summarize* - summarize or aggregate the grouped data. This creates new variables at a *higher* level of grouping. For example we might calculate means with "mean()" or counts with "n()". This results in a smaller, summary table, which we might further summarize or mutate if we want
    
```{r}
# create object
rel_by_region <- gss_sm %>%
  
  # group the rows by bigregion and, within that, by religion
  group_by(bigregion, religion) %>% 
  
  # summarize this table to create a new, much smaller table, with 3 columns: bigregion, religion, and a new summary variable, N, that is a count of the number of observations within each religious group for each region
  summarize(N = n()) %>% 
  
  # with this new table, use the N variable to calculate 2 new columns: relative proportion (freq) and percentage (pct) for each religious category, still grouped by region. Round the results to the nearest percentage point
  mutate(freq = N / sum(N),  # find averages just like in python!
         pct = round((freq*100), 0))

rel_by_region
```

**Comments**
+ Objects on the left side of the pipe "pass through", and whatever is specified on the right of the pipe gets done to that object
+ don't have to keep specifying the name of the underlying data fram object I'm working from
+ everything implicitly carried forward from "gss_sm"
+ Within the pipeline, implicit objects created from summaries and other transformations are carried through also
+ group_by() function sets up how the grouped or nested data will be processed within the summarize(). 

    + Any function used to create a new variable within summarize() (such as mean(), sd(), or n()), will be applied to the *innermost* grouping level first
    + Grouping levels are named from left to right within group_by() from outermost to innermost
    + dplyr summarizes actions and peel off one grouping level at a time, so that the resulting summaries are at the next level up
    + Start with individual-level observations and group them by *religion* within *region*
+ summarize() aggregates the individual observations to counts of the number of people affiliated with each religion, for each region
+ mutate() - adds or removes columns from tables but do not change the grouping level
+ summarize() and mutate() - we can invent named arguments

    + they are the names that the newly created variables in the summary table will have
    
```{r}
rel_by_region %>% 
  group_by(bigregion) %>% 
  summarize(total = sum(pct))
  
```

**Comments**

+ verify the pct values sum to 100 within each region

```{r}
p <- ggplot(rel_by_region, aes(x = bigregion, y = pct, fill = religion))
p + geom_col(position = "dodge2") + 
  labs(x = "Region", y = "Percent", fill = "Religion") + 
       theme(legend.position = "top")
```

**Comments**

+ "dodge2" puts bars side by side

    + default position - proportionally stacked column chart
    + position = "dodge" - stacked within columns (result reads incorrectly)
    + position = "dodge2" - puts the subcategories (religious affiliations) side-by-side within groups (regions)
    
+ still a bad viz. Too many bars side by side. Too crowded
+ dodged charts can be more cleanly expressed as faceted plots
+ faceting removes the need for a legend, thus is simpler for audience to read

```{r}
p <- ggplot(rel_by_region, aes(x = religion, y = pct, fill = religion))
p + geom_col(position = "dodge2") + 
  labs(x = NULL, y = "Percent", fill = "Religion") + 
  guides(fill = FALSE) +
  coord_flip() +
  facet_grid(~ bigregion)
```

**Comments**

+ coord_flip() - switches x and y axes after the plot is made

    + does not remap variables to aesthetics
    
+ dplyr - way to quickly summarize tables of data without having to write code in the body of our ggplot() or geom_ functions

## Continuous Variables by Group or Category

+ Start using "organdata" dataset

```{r}
# look at the data
head(organdata, 10)
vtable(organdata)
summary(organdata)
glimpse(organdata)
organdata %>% select(1:6) %>% sample_n(size = 10)
```

**Comments**

+ Organ procurements rate is a measure of the number of human organs obtained from cadaver organ donors for use in transplant operations
+ Data regarding donation or organs for transplants in 17 OECD countries
+ some missing values ("NA")
+ select() selects columns 1:6

```{r}
# naively plot data and see what happens
# scatterplot of donors vs year
p <- ggplot(organdata, mapping = aes(x = year, y = donors))
p + geom_point()
```

**Comments**

+ if there are heaps of warnings, R will ask me to read them with "warnings()"
+ If want to plot each country's time series using geom_line(), must tell ggplot what the grouping variable is

```{r}
p <- ggplot(organdata, mapping = aes(x = year, y = donors))
p + geom_line(aes(group = country)) +
  facet_wrap(~ country)
```

**Comments**

+ use geom_boxplot() to get a picture of variation by year across countries
+ stat_boxplot() works by default with geom_boxplot() (similar to stat() working with geom_bar())
+ tell geom_boxplot() the variable we want to categorize by "(here, country)" and the continuous variable we want summarized "(here, donors)"

```{r}
p <- ggplot(organdata, mapping = aes(x = country, y = donors))
p + geom_boxplot()
```

**Comments**

+ x-axis is too crowded - names overlap
+ fix this by flipping coords

```{r}
p <- ggplot(organdata, mapping = aes(x = country, y = donors))
p + geom_boxplot() + coord_flip()
```

**Comments**

+ better than previous visual, but not ideal
+ there's no order to this. The audience would be confused
+ reorder(): 2 arguments 

    1. categorical variable or factor that we want to reorder (in this case, country)
    2. variable we want to reorder it by (in this case, donors)
    3. (optional) function wanted to use as a summary statistic (default = reorder categoris of 1st variable by the mean value of 2nd variable)
        + R will fail if using mean() if there are NA's in the data
        + specify it's OK to remove null values (na.rm = TRUE)
        
```{r}
p <- ggplot(organdata, mapping = aes(x = reorder(country, donors, na.rm = TRUE), y = donors))
p + geom_boxplot() + 
  labs(x = NULL) +
  coord_flip()
```

**Comments**

+ since it's obvious these are country names, set labs(x = NULL)

```{r}
p <- ggplot(organdata, mapping = aes(x = reorder(country, donors, na.rm = TRUE), y = donors))
p + geom_violin() + 
  labs(x = NULL) +
  coord_flip()
```

**Comments**

+ same thing as boxplot, but using violin plot instead
+ these are ugly

```{r}
p <- ggplot(organdata, mapping = aes(x = reorder(country, donors, na.rm=TRUE), y = donors, fill = world))
p + geom_boxplot() +
  labs(x = NULL) + 
  coord_flip() + 
  theme(legend.position = "top")
```

**Comments**

+ useful trick - put *categorical variables* on the y-axis to compare their distributions
+ makes it easy to effectively present summary data on more categories
+ geom_point() has argument for "color" but not for "fill" 

```{r}
p <- ggplot(organdata, mapping = aes(x = reorder(country, donors, na.rm = TRUE), y = donors, color = world))
p + geom_point() + 
  labs(x=NULL) + 
  coord_flip() +
  theme(legend.position = "top")
```

**Comments**

+ There are some overplotting of some observations
+ Use geom_jitter() to see points that are plotted on top of each other

```{r}
p <- ggplot(organdata, mapping = aes(x = reorder(country, donors, na.rm=TRUE), y = donors, color = world))
p + geom_jitter() + 
  labs(x=NULL) + 
  coord_flip() + 
  theme(legend.position = "top")
```

**Comments**

+ points are nudge a tiny bit just to show multiple points plotted on the same sport
+ can control amount of "jittering" using "height" and "width" arguments

```{r}
p <- ggplot(organdata, mapping = aes(x = reorder(country, donors, na.rm=TRUE), y = donors, color = world))
p + geom_jitter(position = position_jitter(height = 0.15, width=0.15)) + 
  labs(x=NULL) + 
  coord_flip() + 
  theme(legend.position = "top")
```

**Comments**

+ Did not need "height" argument because it has no effect. 
+ Adding "height" argument results in the same output
+ Use this approach when we want to summarize a categorical variable that just has 1 point per category
+ This method is preferred over bar chart or a table

```{r}
by_country <- organdata %>% 
  group_by(consent_law, country) %>% 
  summarize(donors_mean = mean(donors, na.rm=TRUE), 
            donors_sd = sd(donors, na.rm=TRUE), 
            gdp_mean = mean(gdp, na.rm=TRUE),
            health_mean = mean(health, na.rm=TRUE),
            roads_mean = mean(roads, na.rm=TRUE), 
            cerebvas_mean = mean(cerebvas, na.rm=TRUE))
```

**Comments**

+ 2 steps: 
    
    1. Group data by 'consent_law' and 'country'
    2. Summarize to create 6 new variables, each one of which is the mean or sd of each country's score on a corresponding variable 
    
+ summarize step will inherit information about the original data and gthe grouping and then do its calculations at the innermost grouping level

    + takes all observations for each country and calculates the mean or sd as requested
    
```{r}
by_country
vtable(by_country)
```

**Comments**

+ countries are summarized alphabetically within 'consent_law', which is the outermost grouping variable in the 'group_by()' statement
+ in the pipeline for 'by_county', there are a lot of repeat function calls happening. We can write this more efficiently
+ write a line that applies 'mean()' and 'sd()' functions to every numerical variable in dataset, but *only* numerical ones

```{r}
by_country <- organdata %>% 
  group_by(consent_law, country) %>% 
  summarize_if(is.numeric, funs(mean, sd), na.rm=TRUE) %>% 
  ungroup()
```

**Comments**

+ 'summarize_if()' - examines each column in our data and applies a test to it, only summarizing if the test passes (if return value is TRUE)
+ 'funs()' - list functions we want to apply if return value is TRUE
+ 'ungrou()' - ungroup the data at the end, so that the result is a plain tibble

```{r}
by_country
```

**Comments**

+ all numerical variables have been summarized
+ function applied from 'funs()' is appended to the end of each variable name that it was applied to

    + 'donors_mean'
    + 'donors_sd'
    
```{r}
p <- ggplot(by_country, mapping = aes(x = donors_mean, y = reorder(country, donors_mean), color = consent_law))
p + geom_point(size=3) + 
  labs(x = "Donor Procurement Rate", 
       y = "", color = "Consent Law") + 
  theme(legend.position = "top")
```

**Comments**

+ alternative approach to this; use facet instead of color
+ 'scales = "free_y"' - this usually breaks comparability, but when one axis is categorical, we can free the categorical axis and leave the continuous axis fixed

```{r}
p <- ggplot(by_country, mapping = aes(x = donors_mean, y = reorder(country, donors_mean)))
            
p + geom_point(size=3) + 
  facet_wrap(~ consent_law, scales = "free_y", ncol = 1) + 
  labs(x = "Donor Procurement Rate", 
       y = "")
```

**Comments**

+ Cleveland dotplots are generally preferred to bar or column charts
+ when using these, put the categories on the y-axis and order them in a way that is most relevant to the numerical summary being provided

```{r}
p <- ggplot(by_country, mapping = aes(x = reorder(country, donors_mean), y = donors_mean))

p + geom_pointrange(mapping = aes(ymin = donors_mean - donors_sd, ymax = donors_mean + donors_sd)) +
  labs(x = "", y = "Donor Procurement Rate") + 
  coord_flip()
```

**Comments**

+ 'geom_pointrange()' expects 'y', 'ymin', and 'ymax' as arguments. 
+ We map 'donors_mean' to 'y' and the 'ccode' variable to 'x', 
+ then flip the axes at the end with 'coord_flip()'

## Plot Text Directly

+ 'geom_text()' - use to plot labels along with the points in a scatterplot, or just plot informative labels directly

```{r}
p <- ggplot(by_country, mapping = aes(x = roads_mean, y = donors_mean))
p + geom_point() + 
  geom_text(mapping = aes(label = country))
```

**Comments**

+ 'country' labels are plotted right on top of each point because they are positioned with the same x and y mapping
+ either drop the geom_point or make horizontal adjustment using 'hjust' 

```{r}
p <- ggplot(by_country, mapping = aes(x = roads_mean, y = donors_mean))

p + geom_point() + 
  geom_text(mapping = aes(label = country), hjust = 0)
```

**Comments**

+ 'hjust = 0' - left justify
+ 'hjust = 1' - right justify


```{r}
elections_historic %>% select(2:7)
```

**Comments**

+ Switch datasets to work with some historical US presidential election data via 'socviz' library

```{r}
# could put these labels directly in code, but it is tidier to assign text to objects and pass in the objects
p_title <- "Presidential Elections: Popular & Electoral College Margins"
p_subtitle <- "1824-2016"
p_caption <- "Data for 2016 are provisional."
x_label <- "Winner's share of Popular Vote"
y_label <- "Winner's share of Electoral College Votes"

p <- ggplot(elections_historic, aes(x = popular_pct, y = ec_pct, label = winner_label))

p + geom_hline(yintercept = 0.5, size = 1.4, color = "gray80") + 
  geom_vline(xintercept = 0.5, size = 1.4, color = "gray80") + 
  geom_point() + 
  geom_text_repel() + 
  scale_x_continuous(labels = scales::percent) + 
  scale_y_continuous(labels = scales::percent) + 
  labs(x = x_label, y = y_label, title = p_title, subtitle = p_subtitle, caption = p_caption)
```

**Comments**

+ use 'scale_x_continous()' and 'sclae_y_continuous()' to adjust labels of the scales since shares are stored in the data as proportions (0 to 1) rather than percentages
+ x-axis: popular vote 50% line
+ y-axis: electoral college 50% line
+ 'hline' and 'vline' are plotted at top of code so the points and labels are layered **on top** of them
+ normally it is **NOT** a good idea to label every point on a plot like above. Better approach would be to select a few points of particular interest
+ 'ggrepel' package is consistently favored over using 'geom_text()' 

## Label Outliers

+ to label some points but not others, we must select the points we want to label
+ 'subset()' does the work

```{r}
p <- ggplot(by_country, mapping = aes(x = gdp_mean, y = health_mean))

p + geom_point() + 
  geom_text_repel(data = subset(by_country, gdp_mean > 25000),
                  mapping = aes(label = country))

p <- ggplot(by_country, mapping = aes(x = gdp_mean, y = health_mean))

p + geom_point() + 
  geom_text_repel(data = subset(by_country, 
                                gdp_mean > 25000 | health_mean < 1500 |
                                  country %in% "Belgium"),
                  mapping = aes(label = country))
```

**Comments**

+ 'subset()' - used to create a small dataset on the fly
+ 'subset()' - takes the 'by_country' object and selects only the cases where 'gdp_mean' > 25000

    + only these points are labelled on the plot
    
+ "|" - pipe character represents "or" 

```{r}
organdata$ind <- organdata$ccode %in% c("Ita", "Spa") & organdata$year > 1998

p <- ggplot(organdata, mapping = aes(x = roads, y = donors, color = ind))

p + geom_point() + 
  geom_text_repel(data = subset(organdata, ind),
                  mapping = aes(label = ccode)) + 
  guides(label = FALSE, color = FALSE)
```

**Comments**

+ observation logic - obs is coded as TRUE if ccode is "Ita" or "Spa" **AND** if the 'year' > 1998. 

    + all constraints must be met for observation to be marked as TRUE

## Write and Draw in the Plot Area

+ 'annotate()' - use to point out something important that is not mapped to a variable
    
    + can *use* geoms, temporarily taking advantage of their features in order to place something on the plot
    
```{r}
p <- ggplot(organdata, mapping = aes(x = roads, y = donors))
p + geom_point() + 
  annotate(geom = "text", x = 91, y = 33, 
           label = "A surpringly high \n recovery rate.", 
           hjust = 0)
```

**Comments**

+ I'd rather just use 'ggannotate()' instead

```{r}
p <- ggplot(organdata, 
            mapping = aes(x = roads, y = donors))

p + geom_point() + 
  annotate(geom = "rect", xmin = 125, xmax = 155, 
           ymin = 30, ymax = 35, fill = "red", alpha = 0.2) + 
  annotate(geom = "text", x = 157, y = 33,
           label = "A surprisingly high \n recovery rate.", hjust = 0)
```

**Comments**

+ I'd still rather just use ggannotate() instead

## Understanding Scales, Guides, and Themes

+ 'scale_x_log10()' and 'scale_x_continuous()', and other 'scale_...' functions used to adjust axis labels
+ 'gui9des()' function - remove the legends for a color mapping and a label mapping
+ 'theme()' - move position of  alegend from the side to the top of a figure
+ different plots require different mappings in order to work, and so each 'geom_' function takes mappings tailored to the kind of graph it draws
+ 'scale_' functions: 

    + Every aesthetic mapping has a scale. If you want to adjust how that scale is marked or graduated, then you use a 'scale_' function
    + Many scales come with a legend or key to help the reader interpret the graph. These are called *guides*. You can make adjustments to them with the 'guides()' function. Perhaps the most common use case is to make the legend disappear, as som times it is superfluous. Another is to adjust the arrangement of the key in legends and color bars
    + Graphs have other features not strictly connected to the logical structure of the data being displayed. These include things like their background color, the typeface used for labels, or the placement of the legend on the graph. To adjust these, use the 'theme()' function
    
+ Rule of thumb regarding 'aes()' vs. 'scale_()' and 'geom_()' vs. 'theme()': 

    + If the change you want to make will affect the substantive interpretation of any particular geom, then most likely you will either be mapping an aesthetic to a variable using that geom's 'aes()' function or be specifying a change via some 'scale_' function
    + If the change you want to make does not affect the interpretation of a given 'geom_()', then most likely you will either be setting a variable inside the 'geom_()' function, or making a cosmetic change via the 'theme()' function.
    
```{r}
p <- ggplot(organdata, mapping = aes(x = roads, y = donors, color = world))

p + geom_point()
```

**Comments**

+ every mapped variable has a scale

    + 'roads' mapped to x
    + 'donors' mapped to y
    + 'world' mapped to color
    
+ x and y scales are both *continuous*

    + this is typically the case
    
+ 'world' measure (color or fill scale) is an unordered categorical variable, so its scale is *discrete*

    + this is typically the case
    
+ If x is mapped to a continuous variable, then adding "+ scale_x_continuous()" to the plot statement with no further agruments will have no effect

    + ggplot already understands that the variable is continuous
    + it is already implicitly understood
    
```{r}
p <- ggplot(organdata, mapping = aes(x = roads, y = donors, color = world))
p + geom_point() + 
  scale_x_log10() + 
  scale_y_continuous(breaks = c(5, 15, 25), labels = c("Five", "Fifteen", "Twenty Five"))
```

```{r}
p <- ggplot(organdata, mapping = aes(x = roads, y = donors, color = world))
p + geom_point() + 
  scale_color_discrete(labels = c("Corporatist", "Liberal", "Social Democratic", "Unclassified")) + 
  labs(x = "Road Deaths", 
       y = "Donor Procurement", 
       color = "Welfare State")
```

**Comments**

+ purely cosmetic decisions - use 'theme()' function

```{r}
p <- ggplot(organdata, mapping = aes(x = roads, y = donors, color = world))
p + geom_point() + 
  labs(x = "Road Deaths", 
       y = "Donor Procurement") +
  guides(color = FALSE)
```

**Comments**

+ Removed legend
+ 'scale_()' functions:

    + scale_<mapping>_<kind>()
    
## Where to Go Next

1. Redo fig 5.18 so that it shows all the data points but only labels elections since 1992.See 'elections_historic' dataset to see what variables are available. 

2. Using 'geom_point()' and 'reorder()', make a Cleveland dotplot of all presidential elections, ordered by share of the population vote



3. add a rectangle using 'annotate()' that lightly colors the entire upper left quadrant of figure 5.18



4. Reproduce a pair of graphs from Chapter 1, as seen on 5.28. You will need to filter some rows, group the data by continent, and calculate the mean life expectancy by continent before beinning the plotting process



5. Get comfy with grouping, mutating, and summarizing data in pipelines. Create some grouped objects from the GSS data

```{r}
#gss_sm %>% group_by(race, degree) %>% summarize(N - n()) %>% 
#  mutate(pct = round(N / sum(N) * 100, 0))
```




6. By grouping by 'race' and summing the percentages, check results



7. Calculate mean and median number of children by degree by using functions other than 'sum'



8. Read into 'dplyr' best practices



9. Examine population or life expectancy over time using a series of boxplots using 'gapminder' data. Facet this boxplot by continent. 



10. Experiment with 'notch' and 'varwidth' options for 'geom_boxplot()



11. Swap out 'geom_boxplot()' with 'geom_violin()' to compare



12. Experiment with 'geom_pointrange()' using 'gapminder' or 'organdata' to see how they differ

# Work with Models

```{r}
p <- ggplot(gapminder, mapping = aes(x = log(gdpPercap), y = lifeExp))

p + geom_point(alpha = 0.1) + 
  geom_smooth(color = "tomato", fill = "tomato", method = MASS::rlm) + 
  geom_smooth(color = "steelblue", fill = "steelblue", method = "lm")

p + geom_point(alpha = 0.1) + 
  geom_smooth(color = "tomato", method = "lm", size = 1.2, formula = y ~ splines::bs(x, 3), se = FALSE)

p + geom_point(alpha = 0.1) + 
  geom_quantile(color = "tomato", size = 1.2, method = "rqss", lambda = 1, quantiles = c(0.20, 0.5, 0.85))
```

**Comments**

+ 'stat_' functions compute single numbers or new variables before plotting them for histograms, density plots, boxplots, and other geoms
+ MASS::rlm - 'robust linear model' - fits a robust regression line
+ splines::bs - fit a polynominal curve to the data
+ 'geom_quantile()' - can fit quantil regression lines using a variety of methods

    + quantiles() - takes a vector specifying the quantiles at which to fit the lines
    
## Show Several Fits at Once, with a Legend

+ when plotting multiple layers of 'geom_smooth', always set 'color' and 'fill' to different settings to easily distinguish between each layer
+ it is possible to connect multiple 'geom_smooth' layers and build a legend for it

    + 'scale_color_manual()'
    + 'scale_fill_manual()'

```{r}
model_colors <- RColorBrewer::brewer.pal(3, "Set1")
model_colors  # returns hex values
```

**Comments**

+ returns hex values
+ then, create a plot with 3 different smoothers
+ map color and fill *within the aes() function* as the name of the smoother

```{r}
p0 <- ggplot(gapminder, mapping = aes(x = log(gdpPercap), y = lifeExp))

p1 <- p0 + geom_point(alpha = 0.2) + 
  geom_smooth(method = "lm", aes(color = "OLS", fill = "OLS")) + 
  geom_smooth(method = "lm", formula = y ~ splines::bs(x, df = 3), 
              aes(color = "Cubic Spline", fill = "Cubic Spline")) + 
  geom_smooth(method = "loess", 
              aes(color = "LOESS", fill = "LOESS"))

p1 + scale_color_manual(name = "Models", values = model_colors) + 
  scale_fill_manual(name = "Models", values = model_colors) + 
  theme(legend.position = "top")
```

**Comments**

+ remember that the aes() function is for mapping variables to aesthetics
+ ggplot will properly construct the relevant guide if we call 'scale_color_manual()' and 'scale_fill_manual()'
+ these model-fitting features make ggplot very useful for exploratory work and make it straightforward to generate and compare model-based trends and other summaries as part of the process of descriptive data visualization

## Look Inside Model Objects

+ Key objective always: get from however the object is stored to a tidy table of numbers that we can plot
+ we are always working with objects
+ objects have an internal structure consisting of named pieces

```{r}
gapminder
str(gapminder)
```

**Comments**

+ statistical models in R have an internal structure

```{r}
out <- lm(formula = lifeExp~ gdpPercap + pop + continent, data = gapminder)
```

**Comments**

+ first argument is the formula for the model
+ 'lifeExp' is the dependent variable
+ tilde is used to designate the left and right sides of a model

    + similar to facets, where ~ separates left and right sides
    
```{r}
summary(out)
#str(out)
#out$coefficients
#out$residuals
#out$fitted.values
```

**Comments**

+ like any function, 'summary()' takes its input, performs some actions, and produces output
+ the output above is a mix of information stored inside the model object and partly information that the 'summary()' function has calculated and formatted for display on the screen
+ 'summary()' - selecting and printing only a small amount of core information, in comparison to what is stored in the model object

## Get Model-Based Graphics Right

+ model results typically carry extra burden of interpretation and necessary background knowledge
+ there is no substitute for learning the statistics

### Present your findings in substantive terms

+ Substantive results - showing results in a context where other variables in the analysis are held at sensible values, such as their *means* or *medians*

    + Continuous variables - useful to generate predicted values that cover some substantively meaningful move across the distributions
    + Unordered categorical variables - predicted values might be presented with respect to the modal category in the data
    + often need to convert to a scale that the audience will understand
    
### Show your degree of confidence

+ model estimates come with various measures of precision, confidence, credence, or significance
+ 'geom_ribbon()' - draws filled areas and is useful for plotting ranges of y-axis values along some continuously varying x-axis

### Show your data when you can

+ plotting multivariate models: 

    1. We can show what is in effect a table of coefficients with associated measures of confidence, perhaps organizing the coefficients into meaningful groups, or by the size of the predicted association, or both
    2. We can show the predicted values of some variables (rather than just a model's coefficients) across som range of interest
    
## Generate Predictions to Graph

+ 'predict()' - generic way of using model objects to produce this kind of prediction

    + "generic" functions take their inputs and pass them along to more specific functions behind the scenes, ones that are suited to working with the particular kind of model object we have
    + for 'predict()' to calculate the new values for us, it needs some new data to fit the model to
    + must generate a new data from whose columns have the same names as the variables in the model's original data, but where the rows have new values
    
+ 'expand.grid()' - generates and then will multiply out the full range of values for all combinations of the values we give it, thus creating a new data frame with the new data we need

```{r}
min_gdp <- min(gapminder$gdpPercap)
max_gdp <- max(gapminder$gdpPercap)
med_pop <- median(gapminder$pop)

pred_df <- expand.grid(gdpPercap = (seq(from = min_gdp, to = max_gdp, length.out = 100)), 
                       pop = med_pop, continent = c("Africa", "Americas", "Asia", "Europe", "Oceania"))

dim(pred_df)  ## [1] 500  3

head(pred_df)
```

**Comments**

+ 'dim()' - Dimensions of an Object. Retrieve or set the dimension of an object

```{r}
pred_out <- predict(object = out, newdata = pred_df, interval = "predict")
head(pred_out)
```

**Comments**

+ 'predict()' - if we give the function our new data and model, without any further agrument, it will calculate the fitted values for every row in the data frame
+ using 'interval = "predict"' argument, it wil calculate 95% prediction intervals in addition to the point estimate

```{r}
pred_df <- cbind(pred_df, pred_out)
head(pred_df)
```

**Comments**

+ this is definitely **NOT** best practice
+ this is a tidy data frame

    + contains the predicted values from the model for the range of values we specified
    
```{r}
p <- ggplot(data = subset(pred_df, continent %in% c("Europe", "Africa")), 
            aes(x = gdpPercap, 
                y = fit, ymin = lwr, ymax = upr, 
                color = continent, 
                fill = continent,
                group = continent))

p + geom_point(data = subset(gapminder, continent %in% c("Europe", "Africa")),
               aes(x = gdpPercap, y = lifeExp, 
                   color = continent),
               alpha = 0.5, 
               inherit.aes = FALSE) + 
  geom_line() + 
  geom_ribbon(alpha = 0.2, color = FALSE) + 
  scale_x_log10(labels = scales::dollar)
```

**Comments**

+ OLS predictions
+ 'geom_ribbon()' - ymin and ymax defines the lower and upper limits of the prediction interval
+ 'predict()' - has the ability to work safely with different classes of model that underpines other helper functions in different libraries

## Tidy Model Objects with Broom

+ 'broom' package - library of functions that help us get from the model results that R generates to numbers that we can plot

    + takes model objects and turn pieces of them into data frames that can be used easily with ggplot
    + takes ggplot's approach to tidy data and extends it to the model objects that R produces
    
+ extracts 3 kinds of information: 

    1. *component-level* information about aspects of the model itself, such as coefficients and t-statistics
    2. obtain *observation-l;evel* information about the model's connection to the underlying data
    3. get *model-level* information that summarizes the fit as a whole, such as an F-statistic, the model deviance, or the r-squared
    
### Get component-level statistics with tidy()

+ 'tidy()' - takes a model object and returns a data frame of component-level information

```{r}
out_comp <- tidy(out)
out_comp %>% round_df()
```

**Comments**

+ We are now able to treat this data frame just like all the other data that we have seen so far, and use it to make a plot

```{r}
p <- ggplot(out_comp, mapping = aes(x = term, y = estimate))

p + geom_point() + coord_flip()
```

**Comments**

+ confidence intervals would be nice to know

```{r}
out_conf <- tidy(out, conf.int = TRUE)
out_conf %>% round_df()
```

**Comments**

+ %nin% -  "not in" operator

    + opposite of %in% operator. selects only the items in a first vector of characters that are not in the second
    
+ When fitting a model with categorical variables, R will create coefficient names based on the variable name and category name (e.g. 'continentAmericas')
+ 'prefix_strip()' - drops prefixes to create a new column variable that corresponds to the 'terms' column but has nicer labels

```{r}
out_conf <- subset(out_conf, term %nin% "(Intercept)")
out_conf$nicelabs <- prefix_strip(out_conf$term, "continent")
```

**Comments**

+ 'geom_pointrange()' - displays some information about our confidence in the variable estimates, as opposed to just the coefficients

```{r}
p <- ggplot(out_conf, mapping = aes(x = reorder(nicelabs, estimate), 
                                    y = estimate, ymin = conf.low, ymax = conf.high))
p + geom_pointrange() +
  coord_flip() + 
  labs(x = "", y = "OLS Estimate")
```

**Comments**

+ this displays a nicer plot of OLS estimates and confidence intervals

### Get observation-level statistics with augment()

+ values returned while using 'augment()' are all statistics calculated at the level of the original observations

    + '.fitted' - the fitted values of the model 
    + '.se.fit' - the standard errors of the fitted values
    + '.resid' - the residuals
    + '.hat' - the diagonal errors of the fitted values
    + '.sigma' - an estimate of residual standard deviation when the corresponding observation is dropped from the model
    + '.cooksd' - Cook's distance, a common regression diagnostic
    + '.std.resid' - the standardized residuals
    
```{r}
out_aug <- augment(out)
head(out_aug) %>% round_df()
out_aug <- augment(out, data = gapminder)
head(out_aug) %>% round_df()
```

**Comments**

+ If some rows containing missing data were dropped to fit the model, then these will not be carried over to the augmented data frame

```{r}
p <- ggplot(data = out_aug, mapping = aes(x = .fitted, y = .resid))
p + geom_point()
```

**Comments**

+ possible to plot the residuals vs. fitted values

### Get model-level statistics with glance()

+ 'glance()' - organizes the information typically presented at the bottom of a model's 'summary()' output

```{r}
glance(out) %>% round_df()
```

**Comments**

+ Broom is able to tidy (and augment, and glance at) a wide range of model types

```{r}
out_cph <- coxph(Surv(time, status) ~ age + sex, data = lung)
out_surv <- survfit(out_cph)
```

**Comments**

+ 'Surv()' function - creates the response or outcome variable for the proportional hazards model that is then fitted by the 'coxph()' function
+ 'survfit()' function - creates the survival curve from the model, much like we used 'predict()' to generate predicted values earlier

```{r}
#summary(out_cph)
#summary(out_surv)
out_tidy <- tidy(out_surv)

p <- ggplot(data = out_tidy, mapping = aes(time, estimate))
p + geom_line() + 
  geom_ribbon(mapping = aes(ymin = conf.low, ymax = conf.high), alpha = 0.2)
```

### Grouped Analysis and List Columns

+ 'Broom' makes it possible to quickly fit models to different subsets of your data and get consistent and usable tables of results out the other end
+ gapminder data is organized by 'country-years' - the unit over observation in the rows

```{r}
eu77 <- gapminder %>% 
  filter(continent == 'Europe', year == 1977)

fit <- lm(lifeExp ~ log(gdpPercap), data = eu77)
summary(fit)
```

**Comments**

+ This looks at the gapminder data by examining the relationship between life expectancy and GDP by *continent*, for each year in the data
+ We start with our table of data and then %>% group the countries by 'continent' and 'year' using the 'group_by()' function
+ our data is reorganized first by continent, and within continent by year

```{r}
out_le <- gapminder %>% 
  group_by(continent, year) %>% 
  nest()

out_le
```

**Comments**

+ 'nest()' - resulting object has the tabular form we expect (tibble)
+ *list column* - useful for bundling together complex objects (structured, such as tibbles)
+ look at the data by filtering the data and then *unnesting* the list column

```{r}
out_le %>% filter(continent == "Europe" & year == 1977) %>% 
  unnest()
```

**Comments**

+ *list columns* - useful because we can act on them in a compact and tidy way
+ pass functions along to each row of the *list column* and make something happen

```{r eval=FALSE, include=FALSE}
# helper function to estimate a particular OLS model on some data
#fit_ols <- function(df){
#  lm(lifeExp ~ log(gdpPercap), data = df)
#}

#out_le <- gapminder %>% 
#  group_by(continent, year) %>% 
#  nest() %>% 
#  mutate(model = map(data, fit_ols))

#out_le
```

**Comments**

+ functions are a kind of object
+ **map** - think of "map" as a "for...loop"
+ mapping functions to arrays is more easily integrated into a sequence of data transformations
+ 12/29/2021: error in this chunk. need to debug


```{r eval=FALSE, include=FALSE}
#fit_ols <- function(df){
#  lm(lifeExp ~ log(gdpPercap), data = df)
#}

#out_tidy <- gapminder %>% 
#  group_by(continent, year) %>% 
#  nest() %>% 
#  mutate(model = map(data, fit_ols),
#         tidied = map(model, tidy)) %>% 
#  unnest(tidied, .drop = TRUE) %>% 
#  filter(term %nin% "(Intercept)" &
#           continent %nin% "Oceania")

#out_tidy %>% sample_n(5, replace = TRUE)

```

**Comments**

+ error in my 'sample_n()' function
+ added 'replace = TRUE' as suggested by warning message
+ output is different from the book
+ 12/29/2021: error in this chunk. need to debug

```{r eval=FALSE, include=FALSE}
#p <- ggplot(data = out_tidy, 
#            mapping = aes(x = year, y = estimate, 
#                          ymin = estimate - 2*std.error,
#                          ymax = estimate + 2*std.error,
#                          group = continent, color = continent))

#p + geom_pointrange(position = position_dodge(width = 1)) + 
#  scale_x_continuous(breaks = unique(gapminder$year)) + 
#  theme(legend.position = "top") + 
#  labs(x = "Year", y = "Estimate", color = "Continent")
```

**Comments**

+ The call to 'position_dodge()' within 'geom_pointrange()' allows the point ranges for each continent to be near one another within years, instead of being plotted right on top of one another
+ this approach is good when we're interested in seeing how OLS performs against some other model specification
+ 12/29/2021: error in this chunk. need to debug

## Plot Marginal Effects

+ estimating and plotting *partial* or *marginal effects* from a model has become a common way of presenting accurate and interpretively useful predictions
+ use 'relevel()' function to recode specific factor within a categorical variable to set a reference category

```{r}
gss_sm$polviews_m <- relevel(gss_sm$polviews, ref = "Moderate")

out_bo <- glm(obama ~ polviews_m + sex*race, family = "binomial", data = gss_sm)

summary(out_bo)
```

**Comments**

+ 'glm()' used to fit the model
+ interaction term between 'race' and 'sex' 
+ calculate the marginal effects for each variable using 'margins()' 

```{r}
bo_m <- margins(out_bo)
summary(bo_m)
```

**Comments**

+ using 'margins()', calculate the marginal effects for each variable

```{r}
plot(bo_m)
```

**Comments**

+ using base R, this is an output similar to stata

```{r}
bo_gg <- as_tibble(summary(bo_m))
prefixes <- c("polviews_m", "sex")
bo_gg$factor <- prefix_strip(bo_gg$factor, prefixes)
bo_gg$factor <- prefix_replace(bo_gg$factor, "race", "Race: ")

bo_gg %>% select(factor, AME, lower, upper)
```

**Comments**

+ tidy up the data
+ take results from 'margins()' and plot thme myself
+ convert to a tibble using 'as_tibble()' 
+ use 'prefix_strip()' and 'prefix_replace()' to tidy labels
+ strip 'polviews_m' and 'sex' prefixes and adjust the 'race' prefix
+ this 9 x 4 tibble is now in format that can be plotted

```{r}
p <- ggplot(data = bo_gg, aes(x = reorder(factor, AME), 
                              y = AME, ymin = lower, ymax = upper))

p + geom_hline(yintercept = 0, color = "gray80") + 
  geom_pointrange() + coord_flip() + 
  labs(x = NULL, y = "Average Marginal Effect")
```

**Comments**

+ displays the average marginal effect

```{r}
pv_cp <- cplot(out_bo, x = "sex", draw = FALSE)

p <- ggplot(data = pv_cp, aes(x = reorder(xvals, yvals), 
                              y = yvals, ymin = lower, ymax = upper))

p + geom_hline(yintercept = 0, color = "gray80") + 
  geom_pointrange() + coord_flip() + 
  labs(x = NULL, y = "Conditional Effect")
```

**Comments**

+ Plots just the conditional effects for a particular variable
+ margins package is very powerful. Strongly recommended to add to the quiver

## Plots from Complex Surveys

+ practice using 'survey' functions on 'gss_lon' data

```{r}
options(survey.lonely.psu = "adjust")
options(na.action="na.pass")

gss_wt <- subset(gss_lon, year > 1974) %>% 
  mutate(stratvar = interaction(year, vstrat)) %>% 
  as_survey_design(ids = vpsu, 
                   strata = stratvar,
                   weights = wtssall, 
                   nest = TRUE)
```

**Comments**

+ 2 options set at the beginning provides some information to the 'survey' library about how to behave
+ other tidying operations to get gss_lon data in correct format

```{r}
out_grp <- gss_wt %>% 
  filter(year %in% seq(1976, 2016, by = 4)) %>% 
  group_by(year, race, degree) %>% 
  summarize(prop = survey_mean(na.rm = TRUE))

out_grp
```

**Comments**

+ results returned in 'out_grp' include standard errors
+ it is possible to ask 'survey_mean()' to calculate confidence intervals if I wanted
+ grouping with 'group_by()' lets us calculate counts or means for the innermost variable, grouped by the next variable "up" or "out" 

    + 'degree' is grouped by 'race' such that the proportions for 'degree' will sum to 1 for each group in 'race'
    + this is done separately for each value of 'year'
    
```{r}
out_mrg <- gss_wt %>% 
  filter(year %in% seq(1976, 2016, by = 4)) %>% 
  mutate(racedeg = interaction(race, degree)) %>% 
  group_by(year, racedeg) %>% 
  summarize(prop = survey_mean(na.rm = TRUE))

out_mrg
```

**Comments**

+ 'interaction()' argument produces variable labels that are a compound of the two variables we interacted, with each combination of categories separated by a period
+ see these categories in 2 separate columns (1 for race, 1 for education) - use 'separate()' 


```{r}
out_mrg <- gss_wt %>% 
  filter(year %in% seq(1976, 2016, by = 4)) %>% 
  mutate(racedeg = interaction(race, degree)) %>% 
  group_by(year, racedeg) %>% 
  summarize(prop = survey_mean(na.rm = TRUE)) %>% 
  separate(racedeg, sep = "\\.", into = c("race", "degree"))

out_mrg
```

**Comments**

+ 'separate()' takes the 'racedeg' column, splits each value when it sees a period, and reorganizes the results into 2 columns, 'race' and 'degree'
+ use backslash character as an escape '\\'

```{r}
p <- ggplot(data = subset(out_grp, race %nin% "Other"), 
            mapping = aes(x = degree, y = prop,
                          ymin = prop - 2*prop_se, 
                          ymax = prop + 2*prop_se, 
                          fill = race, 
                          color = race, 
                          group = race))

dodge <- position_dodge(width=0.9)

p + geom_col(position = dodge, alpha = 0.2) + 
  geom_errorbar(position = dodge, width = 0.2) + 
  scale_x_discrete(labels = scales::wrap_format(10)) + scale_y_continuous(labels = scales::percent) + 
  scale_color_brewer(type = "qual", palette = "Dark2") + 
  scale_fill_brewer(type = "qual", palette = "Dark2") + 
  labs(title = "Educational Attanment by Race", 
       subtitle = "GSS 1976-2016",
       fill = "Race",
       color = "Race", 
       x = NULL, y = "Percent") + 
  facet_wrap(~ year, ncol = 2) + 
  theme(legend.position = "top")
```

**Comments**

+ Weighted estimates of educational attainment for white and blacks, GSS selected years 1976-2016
+ faceting bar plots is a bad idea
+ faceting many bar plots is an even worse idea
+ difficult to make comparisons of the categorical bars in each facet
+ 'scales::wrap_format()' wraps the categories plotted on the x-axis
+ 'scales::percent' converts the proportion to a percentage

```{r}
p <- ggplot(data = subset(out_grp, race %nin% "Other"),
            mapping = aes(x = year, y = prop, ymin = prop - 2*prop_se,
                          ymax = prop + 2*prop_se, fill = race, color = race, 
                          group = race))

p + geom_ribbon(alpha = 0.3, aes(color = NULL)) + 
  geom_line() + 
  facet_wrap(~ degree, ncol = 1) + 
  scale_y_continuous(labels = scales::percent) + 
  scale_color_brewer(type = "qual", palette = "Dark2") + 
  scale_fill_brewer(type = "qual", palette = "Dark2") + 
  labs(title = "Educational Attainment by Race", 
       subtitle = "GSS 1976-2016", fill = "Race", 
       color = "Race", x = NULL, y = "Percent") + 
  theme(legend.position = "top")
```

**Comments**

+ dimensions are wack
+ too lazy to troubleshoot this

## Where to Go Next

+ The challenge isn't in plotting the data. The challenege is in calculating and extracting the right numbers
+ it is a requirement that you understand the model you are fitting and the function you use to fit it, especially when: 

    + using interactions
    + using cross-level effects
    + using transformations of the predictor
    + using a response to scales
    

### Default plots for models

+ uses base R or the "lattice" library

```{r}
out <- lm(formula = lifeExp ~ log(gdpPercap) + pop + continent, data = gapminder)

plot(out, which = c(1, 2), ask = FALSE)
```

**Comments**

+ 'which()' - selects the first 2 of 4 default plots for this kind of model
+ "ggfortify" and "coefplot" packages are worth having knowledge on

```{r}
out <- lm(formula = lifeExp ~ log(gdpPercap) + log(pop) + continent, data = gapminder)

coefplot(out, sort = "magnitude", intercept = FALSE)
```

**Comments**

+ neat coefficient plot
+ don't like the grid background 

### Extensions to ggplot

+ GGally package helps make complex plots a little easier
+ generalized pair plots - like a visual correlation matrix
+ only simple when all variables are continuous

```{r}
organdata_sm <- organdata %>% select(donors, pop_dens, pubhealth, roads, consent_law)

ggpairs(data = organdata_sm, mapping = aes(color = consent_law),
        upper = list(continuous = wrap("density"), combo = "box_no_facet"),
        lower = list(continuous = wrap("points"), combo = wrap("dot_no_facet")))
```

**Comments**

+ never use multipanel plots like these in a final presentation
+ used primarily for exploration
+ quickly investigate aspects of a data set

# Draw Maps

+ R can work with geographical data and ggplot can make choropleth maps
+ colorscale often has no midpoint unless defined
+ maps can show data for the same event, but convey different impressions
+ 2 problems: 

    1. Underlying quantities of interest are only partly spatial
        + the number of electoral college votes won and the share of votes cast within a state or county are expressed in spatial terms, but ultimately it is the number of people within thos regions that matter
    2. Regions themselves are of differing sizes and they differ in a way that is not well correlated with the magnitudes of the underlying votes
    
+ Often a map is like a weird grid that you are forced to conform to even though you know it systematically misrepresents what you want to show
+ begin working with "election" dataset

```{r}
# look at the data
head(election, 10)
vtable(election)
summary(election)
```

## Map U.S. State-Level Data

```{r}
election %>% select(state, total_vote, r_points, pct_trump, party, census) %>% 
  sample_n(5)  # sample 5 rows at random
```

**Comments**

+ FIPS code is a federal code that numbers states and territories of the US
+ County level has additional 4 digits
+ each tounty has a unique 6-digit identifier, with the first 2 digist representing the state

```{r}
# Hex color codes for Dem Blue and Rep Red
party_colors <- c("#2E74C0", "#CB454A")

p0 <- ggplot(data = subset(election, st %nin% "DC"), 
             mapping = aes(x = r_points, 
                           y = reorder(state, r_points), 
                           color = party))

p1 <- p0 + geom_vline(xintercept = 0, color = "gray30") +
  geom_point(size = 2)

p2 <- p1 + scale_color_manual(values = party_colors)

p3 <- p2 + scale_x_continuous(breaks = c(-30, -20, -10, 0, 10, 20, 30, 40),
                              labels = c("30\n (Clinton)", "20", "10", "0",
                                         "10", "20", "30", "40\n(Trump)"))

p3 + facet_wrap(~ census, ncol=1, scales="free_y") + 
  guides(color=FALSE) + labs(x = "Point Margin", y = "") + 
  theme(axis.text=element_text(size=8))
```

**Comments**

+ you don't have to represent spatial data spatially
+ this is a state-level dot plot faceted by region
+ the process is broken up by making intermediate objects (p0, p1, p2) along the way
+ when removing "scales = 'free_y'", the y-axis becomes cluttered with state names. It seems that ALL state names are included when 'free_y' argument is omitted
+ when removing the call to 'scale_color_manual()', R uses default color scheme

```{r}
us_states <- map_data("state")
head(us_states)
dim(us_states)  # dimensions of an objects
```

**Comments**

+ look at the data, and find the dimensions of the object
+ this is just a data frame
+ this df has more than 15,000 rows because you need a lot of lines to draw a good-looking map

```{r}
p <- ggplot(data = us_states, mapping = aes(x = long, y = lat, group = group))

p + geom_polygon(fill = "white" , color = "black")
```

**Comments**

+ make a blank state map, using 'geom_polygon()'
+ plotted with lat and long points

    + these are there as scale elements mapped to the x- and y- axes


```{r}
p <- ggplot(data = us_states, aes(x = long, y = lat, group = group, fill = region))

p + geom_polygon(color = "gray90", size = 0.1) + 
  guides(fill = FALSE)
```

**Comments**

+ map 'fill' aesthetic' to 'region' and change the 'color' mapping to a light gray and thin the lines to make the state borders pop pop
+ default is plotted using Mercator projection
+ can transform the default projection used by 'geom_polygon()' via the 'coord_map()' function
+ we typically drawn our plots on a simple Cartesian plane
+ transforming projections means going from sphere object to a flat object or vice versa

```{r}
p <- ggplot(data = us_states, 
            mapping = aes(x = long, y = lat, 
                          group = group, fill = region))

p + geom_polygon(color = "gray90", size = 0.1) + 
  coord_map(projection = "albers", lat0 = 39, lat1 = 45) + 
  guides(fill = FALSE)
```

**Comments**

+ when playing the 'lat0' and 'lat1' values, the map gets stretched and squished
+ to get our own data onto the map, we must merge our data with the underlying df (1500+ rows df)
+ must 'merge' or 'left_join()' our own data with underlying data

    + must know that the values of the key variables you are matching on really do exactly correspond to one another
    
+ merging / left_join is case sensitive

    + convert 'region' variable to lower case using 'tolower()' 
    + use left_join to merge the datasets
    
+ always look at the data when transforming data. never do this blindly
+ maps that look broken when plotted are usually caused by merge errors

```{r}
election$region <- tolower(election$state)

us_states_elec <- left_join(us_states, election)

# look at the data
head(election, 10)
head(us_states_elec, 10)
```

**Comments**

+ how does this chunk know what data I'm referencing?
+ now that everything is in one big data frame, it can be plotted

```{r}
p <- ggplot(data = us_states_elec,
            aes(x = long, y = lat, 
                group = group, fill = party))

p + geom_polygon(color = "gray90", size = 0.1) +
  coord_map(projection = "albers", lat0 = 39, lat1 = 45) + 
  theme(legend.position = "top")
```

**Comments**

+ still need to remove the grid lines and axis labels

```{r}
p0 <- ggplot(data = us_states_elec, 
             mapping = aes(x = long, y = lat, 
                           group = group, fill = party))

p1 <- p0 + geom_polygon(color = "gray90", size = 0.1) +
  coord_map(projection = "albers", lat0 = 39, lat1 = 45)

p2 <- p1 + scale_fill_manual(values = party_colors) + 
  labs(title = "Election Results 2016", fill = NULL)

p2 + theme_map()
```

**Comments**

+ 'theme_map()' throwing error. R can't find this function

```{r}
p0 <- ggplot(data = us_states_elec, 
             mapping = aes(x = long, y = lat, group = group, fill = pct_trump))

p1 <- p0 + geom_polygon(color = "gray90", size = 0.1) + 
  coord_map(projection = "albers", lat0 = 39, lat1 = 45)

p1 + labs(title = "Trump vote") + theme_map() + labs(fill = "Percent")
```

**Comments**

+ had to omit 'theme_map' from p1 + ... line
+ this map draws in default colors (blue) with a gradient on Percent because it's a continuous variable
+ gradient flows in wrong direction - higher % votes should have darker color

```{r}
p2 <- p1 + scale_fill_gradient(low = "white", high = "#CB454A") + 
  labs(title = "Trump vote")

p2 + theme_map() + labs(fill = "Percent")
```

**Comments**

+ omitted 'theme_map()' again
+ changed default color to the hex color (red)
+ set the gradient fill to be more interpretable

```{r}
p0 <- ggplot(data = us_states_elec,
             mapping = aes(x = long, y = lat, group = group, fill = d_points))

p1 <- p0 + geom_polygon(color = "gray90", size = 0.1) + 
  coord_map(projection = "albers", lat0 = 39, lat1 = 45)

p2 <- p1 + scale_fill_gradient2() + labs(title = "Winning margins")
p2 + theme_map() + labs(fill = "Percent")

p3 <- p1 + scale_fill_gradient2(low = "red", mid = scales::muted("purple"),
                                high = "blue", breaks = c(-25, 0, 25, 50, 75)) + 
  labs(title = "Winning margins")

p3 + theme_map() + labs(fill = "Percent")
```

**Comments**

+ DC has highest points margin in favor of the Democrats of any unit of observation in the data
+ If DC is omitted, we'll see that our scale shifts in a way that does not just affect the top of the blue end but recenters the whole gradient and makes the red side more vivid as a result

    + power of omitting data!
    
```{r}
p0 <- ggplot(data = subset(us_states_elec, 
                           region %nin% "district of columbia"), 
             aes(x = long, y = lat, group = group, fill = d_points))

p1 <- p0 + geom_polygon(color = "gray90", size = 0.1) +
  coord_map(projection = "albers", lat0 = 39, lat1 = 45)

p2 <- p1 + scale_fill_gradient2(low = "red", 
                                mid = scales::muted("purple"), 
                                high = "blue") + 
  labs(title = "Winning margins")

p2 + theme_map() + labs(fill = "Percent")
```
    
**Comments**

+ here, we're shoing votes spatially, but what really matters is the number of people who voted in each state

## America's Ur-choropleths

+ Most choropleth maps of the US for whatever variable in effect show population density more than anything else
+ Procedure for drawing maps: 

    1. One data frame containing the map data
    2. One data fram containing the fill variables we want plotted

+ maps regarding county data are large data frames (3k + counties in US)

```{r}
county_map %>% sample_n(5)
```

**Comments**

+ this is df 1
+ 'id' column is FIPS code that we can use for our key

```{r}
county_data %>% 
  select(id, name, state, pop_dens, pct_black) %>% 
  sample_n(5)
```

**Comments**

+ this is df 2
+ 'id' column has extra FIPS codes that are not in df 1. These must be dropped

```{r}
county_full <- left_join(county_map, county_data, by = "id")
```

**Comments**

+ with the data merged, we can map the population density per square mile

```{r}
p <- ggplot(data = county_full, 
            mapping = aes(x = long, y = lat, 
                          fill = pop_dens, 
                          group = group))

p1 <- p + geom_polygon(color = "gray90", size = 0.05) + coord_equal()

p2 <- p1 + scale_fill_brewer(palette="Blues", 
                             labels = c("0-10", "10-50", "50-100", "100-500", 
                                        "500-1,000", "1,000-5,000", ">5,000"))

p2 + labs(fill = "Population per\nsquare mile") + 
  theme_map() + 
  guides(fill = guide_legend(nrow = 1)) + 
  theme(legend.position = "bottom")
```

**Comments**

+ when running 'p1' here, we see that the plot is unordered categorical layout

    + this is because the 'pop_dens' variable is not ordered
    
+ use 'guides()' function to make sure each element of the key appears on the same row
+ use 'scale_fill_brewer()' to manually supply the right sort of scale to make nicer set of labels
+ use 'coord_equal()' to ensure that the relative scale of the map does not change even if we alter the overall dimensions of the plot

```{r}
p <- ggplot(data = county_full, 
            mapping = aes(x = long, y = lat, fill = pct_black, 
                          group = group))

p1 <- p + geom_polygon(color = "gray90", size = 0.05) + coord_equal()

p2 <- p1 + scale_fill_brewer(palette="Greens")

p2 + labs(fill = "US Population, Percent Black") + 
  guides(fill = guide_legend(nrow = 1)) + 
  theme_map() + theme(legend.position = "bottom")
```

**Comments**

+ uses a palette of Greens
+ these maps aren't *explanations* of anything in isolation, but if it turns out that it is more useful to know one or both of these maps instead of the thing I'm plotting, I probably want to reconsider my theory

```{r}
orange_pal <- RColorBrewer::brewer.pal(n = 6, name = "Oranges")
orange_pal
```

**Comments**

+ creates a pallete of Oranges from RCOlorBrewer
+ 'brewer.pal()' function produces evenly spaced color schemes to order from any one of several named palettes
+ colors are in hexadecimal format

```{r}
orange_rev <- rev(orange_pal)
orange_rev
```

**Comments**

+ 'rev()' reverses the order of the palette

```{r}
gun_p <- ggplot(data = county_full, 
                mapping = aes(x = long, y = lat, 
                              fill = su_gun6,
                              group = group))

gun_p1 <- gun_p + geom_polygon(color = "gray90", size = 0.05) + coord_equal()

gun_p2 <- gun_p1 + scale_fill_manual(values = orange_pal)

gun_p2 + labs(title = "Gun-Related Suicides, 1999-2015", 
              fill = "Rate per 100,000 pop.") + 
  theme_map() + theme(legend.position = "bottom")
```

```{r}
pop_p <- ggplot(data = county_full, mapping = aes(x = long, y = lat, 
                                                  fill = pop_dens6,
                                                  group = group))

pop_p1 <- pop_p + geom_polygon(color = "gray90", size = 0.05) + coord_equal()

pop_p2 <- pop_p1 + scale_fill_manual(values = orange_rev)

pop_p2 + labs(title = "Reverse-coded Population Density", 
              fill = "People per square mile") + 
  theme_map() + theme(legend.position = "bottom")
```

**Comments**

+ it's clear that these 2 maps are not identical
+ dark bands in the West (except for California) stand out, and they fade as we move toward the center of the country
+ Northeast has strong similarities too
+ gun-related suicide measure is already expressed as a rate

    + gun-related suicides = deaths in a county / county population
    
+ we normally standardize in this way to "control for" the fact that larger populations will tend to produce more gun-related suicides just because they have more people in them
+ the data is subject to reporting constraints connected to population size
+ 12 deaths in a small population might well put a sparsely populated county in the highest category of suicide rate
+ If estimates for these counties cannot be obtained directly or estimated with a good model, then it is better to drop those cases as missing (even at the cost of your beautiful map), than have large areas of the country painted with a color derived from an unreliable number
+ small differences in reporting, combined with coarse binning and miscoding, will produce spatially misleading and substatively mistaken results
+ Rather than coding in '0' for missing data, a standard alternative is to estimate the suppressed observations using a count model

    + an approach like this might naturally lead to more extensive, properly spatial modeling of the data
    
## Statebins

+ similar to ggplot but slightly different syntax - it needs several arguments, including: 

    1. data frame ('state_data')
    2. vector of state names ('state_col')
    3. value being shown ('value_col')

```{r}
#library(statebins) 

#statebins_continuous(state_data = election, state_col = "state", 
#                     text_color = "white", value_col = "pct_trump", 
#                     brewer_pal="Reds", font_size = 3, 
#                     legend_title="Percent Trump")

#statebins_continuous(state_data = subset(election, st %nin% "DC"), 
#                     state_col = "state", 
#                     text_color = "black", value_col = "pct_clinton", 
#                     brewer_pal="Blues", font_size = 3,
#                     legend_title="Percent Clinton")
```



```{r}
#election <- election %>% mutate(color = recode(party, Republican = "darkred", 
#                                               Democrat = "royalblue"))

#statebins_manual(state_data = election, state_col = "st", 
#                 color_col = "color", text_color = "white", 
#                 font_size = 3, legend_title="Winner", 
#                 labels=c("Trump", "Clinton"), legend_position = "right")

#statebins(state_data = election, 
#          state_col = "state", value_col = "pct_trump", 
#          text_color = "white", breaks = 4,
#          labels = c("4-21", "21-37", "37-53", "53-70"),
#          brewer_pal="Reds", font_size = 3, legend_title="Percent Trump")
```

## Small-Multiple Maps

+ plot small-multiples to show changes over time
+ begin using 'opiates' data

```{r}
# look at the data
head(opiates, 10)
vtable(opiates)
summary(opiates)
str(opiates)
```

**Comments**

+ take our 'us_states' object (with state-level map details) and merge it with opiates dataset

```{r}
opiates$region <- tolower(opiates$state)
opiates_map <- left_join(us_states, opiates)
```

**Comments**

+ As before, we convert the 'state' variable in the 'opiates' data to lowercase first, to make the match work properly
+ since 'opiates' data has a 'year' variable, we can make a faceted small-multiple with one map for each year in the data

```{r}
p0 <- ggplot(data = subset(opiates_map, year > 1999), 
             mapping = aes(x = long, y = lat, 
                           group = group,
                           fill = adjusted))

p1 <- p0 + geom_polygon(color = "gray90", size = 0.05) + 
  coord_map(projection = "albers", lat0 = 39, lat1 = 45)

p2 <- p1 + scale_fill_viridis_c(option = "plasma")

p2 + theme_map() + facet_wrap(~ year, ncol = 3) + 
  theme(legend.position = "bottom", 
        strip.background = element_blank()) + 
  labs(fill = "Death rate per 100,000 population ",
       title = "Opiate Related Deaths by State, 2000-2014")
```

**Comments**

+ cool facet, but too small
+ too lazy to troubleshoot
+ not sure what "size" argument drives
+ choropleth maps of the US tend to track first the size of the local population and secondarily the % of the population that is African American

## Is Your Data Really Spatial?

+ Take our state-level opiates data and redraw it as a time-series plot
+ maintain the state-level focus but try to make the trends more directly visible

```{r}
p <- ggplot(data = opiates, mapping = aes(x = year, y = adjusted, 
                                          group = state))

p + geom_line(color = "gray70")
```

**Comments**

+ a more informative approach is to take advantage of the geographical structure of the data by using the census regions to group the states

```{r}
p0 <- ggplot(data = drop_na(opiates, division_name), 
             mapping = aes(x = year, y = adjusted))

p1 <- p0 + geom_line(color = "gray70", 
                     mapping = aes(group = state))

# add smoother
p2 <- p1 + geom_smooth(mapping = aes(group = division_name), 
                       se = FALSE)
```

**Comments**

+ 'drop_na()' function delets rows that have observations missing on the specified variables
+ we map the 'group' aesthetic to 'state' in 'geom_line(), which gives us a line plot for every state
+ use the 'color' argument to set the lines to a light gray
+ for smoother: 

    + set 'group' aesthetic' to 'division_name'
    + if this is set to 'state', we would get 50 separate smoothers in addition to our 50 trend lines

```{r}
p3 <- p2 + geom_text_repel(data = subset(opiates, 
                                         year == max(year) & abbr !="DC"),
                           mapping = aes(x = year, y = adjusted, label = abbr), 
                           size = 1.8, segment.color = NA, nudge_x = 30) + 
  coord_cartesian(c(min(opiates$year), 
                    max(opiates$year)))
```

**Comments**

+ use 'geom_text_repel()' because we are labeling lines rather than points, we want the state label to appear only at the end of the line
+ use 'segment.color = NA' to turn off the little line segments that indicate what the labels refer to 
+ use 'nudge_x' to nudge labels off to the right of the lines a little
+ use 'coord_cartesian()' to set the axis limits so that there is enough room for labels

```{r}
p3 + labs(x = "", y = "Rate per 100,000 population", 
          title = "State-Level Opiate Dath Rates by Census Division, 1999-2014") + 
  facet_wrap(~ reorder(division_name, -adjusted, na.rm = TRUE), nrow = 3)
```

**Comments**

+ facet the results by census division and add our labels
+ use '-adjusted' to reverse the order of this data
+ this shows a clearer picture compared to the maps
+ now able to see the climbing numbers in NH, RI, MA, CN
+ can easily see state-level differences in the West
+ easily see rapid rise in WV death rate
TS plots are better at conveying the diverging trajectories of various states within regions

## Where to Go Next

+ This chapter taught me how to begin to work with state-level and county-level data organized by FIPS codes
+ GIS historically not conveniently connected to software oriented to the analysis of tabular data
+ many other resources to learn more about mapping in R using various libraries and packages

# Refine your Plots

+ ggplot default settings work fine for data exploration, even final product sometimes
+ only need to customize when you have to 
+ always be mindful of audience - formatting for a journal, conference audience, or general public
+ start working with 'asasec' data

```{r}
# look at the data
head(asasec, 10)
vtable(asasec)
summary(asasec)
str(asasec)
```

**Comments**

+ we have membership data for each section over a 10 year period

```{r}
p <- ggplot(data = subset(asasec, Year == 2014), mapping = aes(x = Members, y = Revenues, label = Sname))

p + geom_point() + geom_smooth()
```

**Comments**

+ basic scatterplot-and-smoother graph
+ let's refine it!

    + identify outliers
    + switch from 'loess' to 'OLS' method
    + introduce 3rd variable

```{r}
p <- ggplot(data = subset(asasec, Year == 2014), mapping = aes(x = Members, y = Revenues, label = Sname))

p + geom_point(mapping = aes(color = Journal)) + geom_smooth(method = "lm")
```

**Comments**

+ better, but now need some text labels
+ at this point, makes sense to build intermediate objects and layer up as I go

```{r}
p0 <- ggplot(data = subset(asasec, Year == 2014), mapping = aes(x = Members, y = Revenues, label = Sname))

p1 <- p0 + geom_smooth(method = "lm", se = FALSE, color = "gray80") + 
  geom_point(mapping = aes(color = Journal))

p2 <- p1 + geom_text_repel(data = subset(asasec, Year == 2014 & Revenues > 7000), size = 2)
```

**Comments**

+ p0 - bones of plot
+ p1 - scatter plot and smoother
+ p2 - adds text labels on specific criteria

```{r}
p3 <- p2 + labs(x="Membership", 
                y="Revenues", 
                color = "Section has own Journal", 
                title = "ASA Sections", 
                subtitle = "2014 Calendar year.", 
                caption = "Source: ASA annual report.")

p4 <- p3 + scale_y_continuous(labels = scales::dollar) + 
  theme(legend.position = "bottom") 

print(p4)
```

**Comments**

+ p3 - add labels to axes, add colors, add titles, add caption
+ p4 - change scale labels and move legend to bottom

## Use Color to Your Advantage

+ color should be aligned with its ability to express the data that is being plotted
+ distinct colors that won't be easily confused with one another to represent categorical data
+ do not map sequential scales to categorical palettes
+ do not use a diverging palette for a variable with no well-defined midpoint
+ ggplot default color palettes typically are good enough
+ 'RColorBrewer' package usually will have what you need

```{r}
p <- ggplot(data = organdata, mapping = aes(x = roads, y = donors, color = world))

p + geom_point(size = 2) + scale_color_brewer(palette = "Set2") + theme(legend.position = "top")

p + geom_point(size = 2) + scale_color_brewer(palette = "Paste12") + theme(legend.position = "top")

p + geom_point(size = 2) + scale_color_brewer(palette = "Dark2") + theme(legend.position = "top")
```

**Comments**

+ each version of 'p' above shows different color palettes

```{r}
demo('colors')
```

**Comments**

+ overview of colors that R would know about if named in "quotation marks" 
+ can also manually specify colors using hexadecimal RGB values

    + always begins with pound character "#"
    + followed by 3 pairs of "hex" numbers (6 altogether)
    + read as "#rrggbb"
    
```{r}
cb_palette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

p4 + scale_color_manual(values = cb_palette)
```

**Comments**

+ 'dichromat' package has choices for safe palettes to use for color-blind viewers

```{r}
Default <- brewer.pal(5, "Set2")
```

**Comments**

+ this gives us 5 colors from ggplot's default palette
+ use 'dichromat' package to transform these colors to new values that simulate different kinds of color blindness

```{r}
types <- c("deutan", "protan", "tritan")
names(types) <- c("Deuteronopia", "Protanopia", "Tritanopia")

color_table <- types %>% 
  purrr::map(~dichromat(Default, .x)) %>% 
  as_tibble() %>% 
  add_column(Default, .before = TRUE)

color_table
color_comp(color_table)
```

**Comments**

+ created a vector of 'types' of color blindess that the 'dichromat()' functions knows about and give them proper names
+ made a table of colors for each type using 'purrr::map()'
+ remainder of the code converts the results from a list to a tibble and adds the original colors as the first column in the table
+ plot to make comparisons
+ do not use stereotypical colors just because I can

## Layer Color and Text Together

+ work through an example using 2016 presidential election results

```{r}
# Democrat Blue and Republican Red
party_colors <- c("#2E74C0", "#CB454A")

p0 <- ggplot(data = subset(county_data, 
                           flipped == "No"), 
             mapping = aes(x = pop, 
                           y = black/100))

p1 <- p0 + geom_point(alpha = 0.15, color = "gray50") + 
  scale_x_log10(labels=scales::comma)

p1
```

**Comments**

+ begin by defining blue and red colors for Dems and Reps
+ Create basic setup and first layer of the plot
+ subset the data, including only counties with a value of "No" on the 'flipped' variable
+ set the scatter plot geom_point() layer to light gray since this is the background of our plot
+ apply log transformation to x-axis scale

```{r}
p2 <- p1 + geom_point(data = subset(county_data, 
                                    flipped == "Yes"),
                      mapping = aes(x = pop, y = black/100, 
                                    color = partywinner16)) + 
  scale_color_manual(values = party_colors)

p2
```

**Comments**

+ add a second geom_point() layer
+ utilize same dataset but extract a complementary subset from it
+ choose "Yes" counties on the 'flipped' variable
+ add color to x and y points, mapping the 'partywinner16' variable to the 'color' aesthetic
+ specify our custom colors using 'scale_color_manual()' 

```{r}
p3 <- p2 + scale_y_continuous(labels=scales::percent) + 
  labs(color = "County flipped to ...", 
       x = "County Population (log scale)", 
       y = "Percent Black Population", 
       title = "Flipped counties, 2016", 
       caption = "Counties in gray did not flip.")

p3
```

**Comments**

+ this layer sets the y-axis scale and the labels

```{r}
p4 <- p3 + geom_text_repel(data = subset(county_data,
                                         flipped == "Yes" & black > 25), 
                           mapping = aes(x = pop, 
                                         y = black/100, 
                                         label = state), size = 2)

p4 + theme_minimal() +
  theme(legend.position="top")
```

**Comments**

+ We are interested in the flipped counties that have a relatively high percentage of African American residents
+ Key questions to ask myself when viewing a visual: 

    + Can I see the mapping that form the basis of the plot?
    + Which variables are mapped to x and y?
    + What geoms were used to produce them?
    + How have the scales been adjusted?
    + Are the axes transformed?
    + Are fill and color legends combined?
    + What is the base layer?
    + What has been drawn on top of the base layer and in what order?
    + Which upper layers are formed from subsets of the data?
    + Which layers are new datasets?
    + Which layers are annotations?

+ Being able to answer these questions is useful for looking at plots and for thinking about how to make them myself

## Change the Appearance of Plots with Themes

+ themes can be turned on or off using the 'theme_set()' function

```{r}
# apply different themes
theme_set(theme_bw())
p4 + theme(legend.position = "top")

theme_set(theme_dark())
p4 + theme(legend.position = "top")
```

**Comments**

+ theme functions are a set of detailed instructions to turn on, turn off, or modify a large number of graphical elements on the plot
+ once set, a theme applies to all subsequent plots and remains active until it is replaced by a different theme
+ Possible to define your own themese either entirely from scratch or by starting with one you like and making adjustments from there

```{r}
theme_set(theme_economist())
p4 + theme(legend.position="top")

theme_set(theme_wsj())

p4 + theme(plot.title = element_text(size = rel(0.6)), 
           legend.title = element_text(size = rel(0.35)), 
           plot.caption = element_text(size = rel(0.35)), 
           legend.position = "top")
```

**Comments**

+ in general, themes with colored background and customized typefaces are best used when making one-off graphics or posters, preparing figures to integrate into a slide presentation, or conforming to a house or editorial style of publication
+ just using defaults or classic typically works good enough

```{r}
p4 + theme(legend.position="top")

p4 + theme(legend.position="top", 
           plot.title = element_text(size=rel(2), 
                                     lineheight=.5, 
                                     family="Times", 
                                     face="bold.italic", 
                                     colour="orange"), 
           axis.text.x = element_text(size=rel(1.1), 
                                      family="Courier", 
                                      face="bold", 
                                      color="purple"))
```

**Comments**

+ control and tweak details of the theme like so

## Use Theme Elements in a Substantive Way

+ It makes good sense to use themes as a way to fix design elements because that means you can subsequently ignore them and focus instead on the data you are examining
+ begin working with 'gss_lon' data



```{r}
# look at the data
head(gss_lon, 10)
vtable(gss_lon)
summary(gss_lon)
str(gss_lon)
```

**Comments**

+ 'gss_lon' data contains information on the age of each GSS respondent for all the years in the survey since 1972

```{r}
yrs <- c(seq(1972, 1988, 4), 1993, seq(1996, 2016, 4))

mean_age <- gss_lon %>% 
  filter(age %nin% NA && year %in% yrs) %>% 
  group_by(year) %>% 
  summarize(xbar = round(mean(age, na.rm = TRUE), 0))
mean_age$y <- 0.3

yr_labs <- data.frame(x = 85, y = 0.8, year = yrs)
```

**Comments**

+ Calculate the mean age of the respondents for each year of interest
+ Look at distributions about every 4 years since the beginning
+ the 'y' column in 'mean_age' will come in handy when we want to position the age as a text label

```{r}
# prepare the data and setup the geoms

p <- ggplot(data = subset(gss_lon, year %in% yrs), 
            mapping = aes(x = age))

p1 <- p + geom_density(fill = "gray20", color = FALSE, 
                       alpha = 0.9, mapping = aes(y = ..scaled..)) + 
  geom_vline(data = subset(mean_age, year %in% yrs), 
             aes(xintercept = xbar), color = "white", size = 0.5) + 
  geom_text(data = subset(mean_age, year %in% yrs), 
            aes(x = xbar, y = y, label = xbar), nudge_x = 7.5, 
            color = "white", size = 3.5, hjust = 1) + 
  geom_text(data = subset(yr_labs, year %in% yrs), 
            aes(x = x, y = y, label = year)) + 
  facet_grid(year ~ ., switch = "y")
```

**Comments**

+ initial 'p' object subsets the data by the years we have chosen and maps 'x' to the 'age' variable
+ 'geom_density()' call is the base layer, with arguments to turn off its default line color, set the fill to a shade of gray, and scale the y-axis between 0 and 1
+ 'geom_vline()' uses the summarized dataset and draws a vertical line at the mean age of the distribution
+ 'nudge' used to push the text slightly to the right of the x-value
+ 'facet_grid()' to break out the age distributions by year
+ 'switch' argument moves labels to the left

```{r eval=FALSE, include=FALSE}
#theme_set(hrbrthemes::theme_book())

#p1 + theme_book(base_size = 10, plot_title_size = 10, 
#                strip_text_size = 32, panel_spacing = unit(0.1, "lines")) + 
#  theme(plot.title = element_text(size = 16), 
#        axis.text.x = element_text(size = 12), 
#        axis.title.y = element_blank(),
#        axis.text.y = element_blank(),
#        axis.ticks.y = element_blank(),
#        strip.background = element_blank(),
#        strip.text.y = element_blank(),
#        panel.grid.major = element_blank(),
#        panel.grid.minor = element_blank()) + 
#  labs(x = "Age", 
#       y = NULL, 
#       title = "Age Distribution of \nGSS Respondents")
```

**Comments**

+ 'theme_book()' function could not be found. Need to debug this

    + hrbrthemes library already loaded. what gives?
    
```{r}
p <- ggplot(data = gss_lon, 
            mapping = aes(x = age, y = factor(year, levels = rev(unique(year)), 
                                              ordered = TRUE)))

p + geom_density_ridges(alpha = 0.6, fill = "lightblue", scale = 1.5) + 
  scale_x_continuous(breaks = c(25, 50, 75)) + 
  scale_y_discrete(expand = c(0.01, 0)) + 
  labs(x = "Age", y = NULL, 
       title = "Age Distribution of \nGSS Respondents") + 
  theme_ridges() + 
  theme(title = element_text(size = 16, face = "bold"))
```

**Comments**

+ this is not like the book at all
+ this visual is terrible
+ 'expand' argument in 'scale_y_discrete()' adjusts the scaling of the y-axis slightly
+ making adjustments to themes should be the last thing to spend time on in plotting process
+ once a theme is found that works well, time shouldn't be spent making minor adjustments that make little to no impact

## Case Studies

+ bad graphics are everywhere. better ones are within my reach now

### Two y-axes

+ S&P 500 stock market - 700-2,100 over the period of interest (last 7 years)
+ Monetary Base - measure of the size of money supply

    + range from 1.5 trillion - 4.1 trillion $ over last 7 years
    
+ when people draw plots with 2 y-axes, they want to line the series up as closely as possible because they suspect that there's a substantive association between them
+ main problem with using 2 y-axes is that it makes it easy to fool yourself (and others) about the degree of association between the variables
+ if the seres are not in the same units (or of widely differing magnitudes), rescale one of the series (e.g., by dividing or multiplying it by 1000)

    + alternatively to index each series to 100 at the start of the first period and then plot them both
    
+ begin working with 'fredts' data

```{r}
# look at the data
head(fredts, 10)
vtable(fredts)
summary(fredts)
str(fredts)
```

**Comments**

+ it's a time series!

```{r}
fredts_m <- fredts %>% select(date, sp500_i, monbase_i) %>% 
  gather(key = series, value = score, sp500_i:monbase_i)

head(fredts_m, 10)
```

**Comments**

+ Now that data is tidied in this format, we can make a plot

```{r}
p <- ggplot(data = fredts_m, 
            mapping = aes(x = date, y = score, 
                          group = series, 
                          color = series))
p1 <- p + geom_line() + theme(legend.position = "top") + 
  labs(x = "Date", 
       y = "Index", 
       color = "Series")

p <- ggplot(data = fredts, 
            mapping = aes(x = date, y = sp500_i - monbase_i))

p2 <- p + geom_line() + 
  labs(x = "Date", 
       y = "Difference")
```

**Comments**

+ here, we are dealing with 3 series
    
    + 2 indices
    + 1 difference between them
    
+ approach: make 2 separate plots and arrange how I like. 

```{r}
theme_set(theme_classic())
cowplot::plot_grid(p1, p2, nrow = 2, rel_heights = c(0.75, 0.25), align = "v")
```

**Comments**

+ why is this plot using this theme? Looks like either WSJ or HBR theme (maybe NY Times)
+ S&P Index runs above the Monetary Base for almost the entire series

    + this is different from the plots provided in the book (the series crossed many times)
    
+ correlation != causation, especially with TS data
+ TS might have seasonal component that we would want to account for prior to making claims about growth
+ to make predictions using TS data, need to make the serial nature of the data go away

### Redrawing a bad slide

+ use some Yahoo slide as an example
+ start working with 'yahoo' data

```{r}
# look at the data
head(yahoo, 10)
vtable(yahoo)
summary(yahoo)
str(yahoo)
```

```{r}
p <- ggplot(data = yahoo,
            mapping = aes(x = Employees, y = Revenue))

p + geom_path(color = "gray80") + 
  geom_text(aes(color = Mayer, label = Year),
            size = 3, fontface = "bold") + 
  theme(legend.position = "bottom") + 
  labs(color = "Mayer is CEO", 
       x = "Employees", y = "Revenue (Millions)", 
       title = "Yahoo Employees vs Revenues, 2004-2014") + 
  scale_y_continuous(labels = scales::dollar) + 
  scale_x_continuous(labels = scales::comma)
```

**Comments**

+ this plot suggests that Mayer was appointed *after* a period of falling revenues and just following a very large round of layoffs

```{r}
p <- ggplot(data = yahoo, 
            mapping = aes(x = Year, y = Revenue / Employees))

p + geom_vline(xintercept = 2012) + 
  geom_line(color = "gray60", size = 2) + 
  annotate("text", x = 2013, y = 0.44, 
           label = " Mayer becomes CEO", size = 2.5) + 
  labs(x = "Year\n", 
       y = "Revenue/Employees", 
       title = "Yahoo Revenue to Employee Ratio, 2004-2014")
```

**Comments**

+ kept time on x-axis
+ plots the ratio of revenue to employees on the y-axis
+ vertical line marks Mayer's accession to CEO

### Saying no to pie

+ Cleveland dotplot or bar chart are usually much more straightforward to compare quantities over a pie chart
+ compromise made to facilitate interpretation is to display all the numerical values for every wedge, and also to add a summary outside the pie

    + typically more effective to show a table instead of a pie chart
    
+ begin working with 'studebt' data set

```{r}
# look at the data
head(studebt, 10)
vtable(studebt)
summary(studebt)
str(studebt)
```

```{r}
# set up labels in advance to reuse later
p_xlab <- "Amount Owed, in thousands of Dollars"
p_title <- "Outstanding Student Loans"
p_subtitle <- "44 million borrowers owe a total of $1.3 trillion"
p_caption <- "Source: FRB NY"

f_labs <- c(`Borrowers` = "Percent of\nall Borrowers", 
            `Balances` = "Percent of\nall Balances")

p <- ggplot(data = studebt, 
            mapping = aes(x = Debt, y = pct/100, fill = type))
p + geom_bar(stat = "identity") + 
  scale_fill_brewer(type = "qual", palette = "Dark2") + 
  scale_y_continuous(labels = scales::percent) + 
  guides(fill = FALSE) + 
  theme(strip.text.x = element_text(face = "bold")) + 
  labs(y = NULL, x = p_xlab, 
       caption = p_caption,
       title = p_title, 
       subtitle = p_subtitle) + 
  facet_grid(~ type, labeller = as_labeller(f_labs)) + 
  coord_flip()
```

**Comments**

+ relabeled facets to something more informative than bare variable names

    + accomplished by using the 'abeller' argument and the 'as_labeller()' function inside the 'facet_grid()' call

+ 'f_labs' - essentially a tiny data frame that associates new labels with the values of the 'type' variable in 'studebt'

    + backticks (tick mark on same key as ~ next to '1' and 'esc') used to pick out the values we want to relabel
    + 'as_labeller()' function takes 'f_labs' object and uses it to create new text for the labels when 'facet_grid()' is called 

+ better than pie chart:

    + split the data into 2 categories and showed the percentage of shares as bars
    + % scores on x-axis
    + values on y-axis to distinguish debt categories
        + easily comparable!
        
+ when axis labels are long, easier to read them on y-axis (hence coord_flip())

```{r}
p <- ggplot(studebt, aes(y = pct/100, x = type, fill = Debtrc))

p + geom_bar(stat = "identity", color = "gray80") + 
  scale_x_discrete(labels = as_labeller(f_labs)) + 
  scale_y_continuous(labels = scales::percent) + 
  scale_fill_viridis(discrete = TRUE) + 
  guides(fill = guide_legend(reverse = TRUE, 
                             title.position = "top", 
                             label.position = "bottom", 
                             keywidth = 3, 
                             nrow = 1)) + 
  labs(x = NULL, y = NULL,
       fill = "Amount Owed, in thousands of dollars", 
       caption = p_caption, 
       title = p_title, 
       subtitle = p_subtitle) + 
  theme(legend.position = "top", 
        axis.text.y = element_text(face = "bold", hjust = 1, size = 12), 
        axis.ticks.length = unit(0, "cm"), 
        panel.grid.major.y = element_blank()) + 
  coord_flip()
```

**Comments**

+ 'guides()' call does a lot of work: 

    + 'fill' mapping: reverse the direction of the color coding
    + 'title.position': put the legend title above the key
    + 'label.position': put the labels for the colors below the key
    + 'keywidth': widen the width of the color boxes a little
    + 'nrow': place the whole key on a single row
    
+ It is difficult to estimate sizes when we don't have an anchor point or baseline scale to compare each piece to

### Where to Go Next

+ Become more confident and practiced with my coding

    + what I choose to pursue will (and should)_ be drivey by my own needs and interests as a scholar and data scientist
    + work through R4DS next (already doing it for Bosanko!)

+ the grammar of graphics is a model that can be used to look at and interpret *any* graph, no matter how it was produced
+ GG provides a vocabulary that lets me say what the data, mappings, geoms, scales, guides, and layers of any particular graph might be


